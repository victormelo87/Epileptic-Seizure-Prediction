{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ff09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyedflib antropy scikit-learn numpy scipy pandas google-api-python-client google-auth-httplib2 google-auth-oauthlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96c38a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import io\n",
    "import gc\n",
    "import pyedflib\n",
    "import pandas as pd\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import skew, kurtosis\n",
    "import antropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0a053a",
   "metadata": {},
   "source": [
    "Funções de Acesso ao Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cf3f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCOPES = ['https://www.googleapis.com/auth/drive.readonly']\n",
    "\n",
    "def get_drive_service():\n",
    "    creds = None\n",
    "    creds_folder = 'credentials'\n",
    "    token_path = os.path.join(creds_folder, 'token.json')\n",
    "    credentials_path = os.path.join(creds_folder, 'credentials.json')\n",
    "\n",
    "    os.makedirs(creds_folder, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(token_path):\n",
    "        creds = Credentials.from_authorized_user_file(token_path, SCOPES)\n",
    "\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            if not os.path.exists(credentials_path):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"ERRO CRÍTICO: O arquivo 'credentials.json' não foi encontrado dentro da pasta '{creds_folder}'.\"\n",
    "                )\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(credentials_path, SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "\n",
    "        with open(token_path, 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    try:\n",
    "        service = build('drive', 'v3', credentials=creds)\n",
    "        print(\"Serviço do Google Drive conectado com sucesso.\")\n",
    "        return service\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao construir o serviço do Drive: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_folder_id(service, folder_name, parent_id='root'):\n",
    "    query = f\"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and '{parent_id}' in parents\"\n",
    "    results = service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
    "    items = results.get('files', [])\n",
    "    return items[0]['id'] if items else None\n",
    "\n",
    "\n",
    "def find_folder_id_by_path(service, path_components):\n",
    "    current_parent_id = 'root'\n",
    "    for folder_name in path_components:\n",
    "        query = f\"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and '{current_parent_id}' in parents\"\n",
    "        results = service.files().list(q=query, fields=\"files(id)\").execute()\n",
    "        items = results.get('files', [])\n",
    "        if not items:\n",
    "            print(f\"Pasta '{folder_name}' não encontrada em '{current_parent_id}'.\")\n",
    "            return None\n",
    "        current_parent_id = items[0]['id']\n",
    "    print(\"Caminho do dataset encontrado no Drive!\")\n",
    "    return current_parent_id\n",
    "\n",
    "\n",
    "def get_files_from_drive_folder(service, folder_id):\n",
    "    query = f\"'{folder_id}' in parents\"\n",
    "    results = service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
    "    return {file['name']: file['id'] for file in results.get('files', [])}\n",
    "\n",
    "\n",
    "def download_file_locally(service, file_id, local_filename):\n",
    "    request = service.files().get_media(fileId=file_id)\n",
    "    with io.FileIO(local_filename, 'wb') as fh:\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "            if status:\n",
    "                print(f\"Download {int(status.progress() * 100)}% concluído...\", end=\"\\r\")\n",
    "    print(f\"Arquivo salvo em: {local_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1f237",
   "metadata": {},
   "source": [
    "Funções de Processamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c49426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_summary_file(file_path):\n",
    "    seizure_info = {}; current_file = \"\"\n",
    "    with open(file_path, 'r', errors='ignore') as f: lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"File Name:\"): \n",
    "            current_file = line.split(': ')[1]; seizure_info[current_file] = []\n",
    "        elif line.startswith(\"Seizure Start Time:\") and i + 1 < len(lines):\n",
    "            start_time = int(re.search(r'(\\d+)', line).group(1)); end_line = lines[i+1].strip()\n",
    "            if end_line.startswith(\"Seizure End Time:\"): \n",
    "                end_time = int(re.search(r'(\\d+)', end_line).group(1))\n",
    "                seizure_info[current_file].append((start_time, end_time))\n",
    "    return seizure_info\n",
    "\n",
    "def extract_single_feature_vector(eeg_window, fs=256):\n",
    "    freqs, psd = welch(eeg_window, fs=fs, nperseg=len(eeg_window)); total_power = np.sum(psd)\n",
    "    def get_band_power(f_low, f_high): return np.sum(psd[np.logical_and(freqs >= f_low, freqs <= f_high)])\n",
    "    delta, theta, alpha, beta, gamma = get_band_power(0.5, 4), get_band_power(4, 8), get_band_power(8, 13), get_band_power(13, 30), get_band_power(30, 80)\n",
    "    band_powers = [p / total_power if total_power > 0 else 0 for p in [delta, theta, alpha, beta, gamma]]\n",
    "    ratios = [beta / alpha if alpha > 0 else 0, (delta + theta) / (alpha + beta) if (alpha + beta) > 0 else 0]\n",
    "    entropies = [antropy.perm_entropy(eeg_window, normalize=True), antropy.spectral_entropy(eeg_window, sf=fs, method='welch', normalize=True), antropy.sample_entropy(eeg_window)]\n",
    "    stats = [np.mean(np.abs(eeg_window)), np.std(eeg_window), skew(eeg_window), kurtosis(eeg_window)]\n",
    "    features = [total_power] + band_powers + ratios + entropies + stats\n",
    "    if len(features) < 46: features.extend([np.mean(features) if features else 0] * (46 - len(features)))\n",
    "    return np.array(features[:46])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d93fc",
   "metadata": {},
   "source": [
    "Classe do Modelo HDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e7285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class HDC:\n",
    "    def __init__(self, dimensions, num_features, num_levels, num_classes=2, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.D, self.num_features, self.num_levels, self.num_classes = dimensions, num_features, num_levels, num_classes\n",
    "        self.level_vectors = np.random.choice([-1, 1], size=(num_levels, self.D))\n",
    "        self.feature_vectors = np.random.choice([-1, 1], size=(num_features, self.D))\n",
    "        self.class_prototypes = np.zeros((self.num_classes, self.D))\n",
    "\n",
    "    def _quantize(self, data, num_levels):\n",
    "        min_val, max_val = np.min(data), np.max(data)\n",
    "        if max_val == min_val: return np.zeros_like(data, dtype=int)\n",
    "        return np.round((data - min_val) / (max_val - min_val) * (num_levels - 1)).astype(int)\n",
    "\n",
    "    def encode(self, x_data):\n",
    "        num_samples, num_features = x_data.shape\n",
    "        x_quantized = np.array([self._quantize(x_data[:, i], self.num_levels) for i in range(num_features)]).T\n",
    "        encoded_data = np.zeros((num_samples, self.D))\n",
    "        for i in range(num_samples):\n",
    "            sample_hv = np.sum([self.feature_vectors[f] * self.level_vectors[x_quantized[i, f]] for f in range(num_features)], axis=0)\n",
    "            encoded_data[i] = np.sign(sample_hv) if np.any(sample_hv) else np.zeros(self.D)\n",
    "        return encoded_data\n",
    "\n",
    "    def predict(self, x_encoded):\n",
    "        similarities = cosine_similarity(x_encoded, self.class_prototypes)\n",
    "        return np.argmax(similarities, axis=1)\n",
    "\n",
    "    def train_standard(self, x_encoded, y_train):\n",
    "        self.class_prototypes = np.array([np.sum(x_encoded[y_train == i], axis=0) for i in range(self.num_classes)])\n",
    "\n",
    "    def train_multipass(self, x_encoded, y_train, epochs=10, lr=0.05, subtract_wrong=True, initial_training=True):\n",
    "        if initial_training: self.train_standard(x_encoded, y_train)\n",
    "        for _ in range(epochs):\n",
    "            y_pred = self.predict(x_encoded)\n",
    "            for i in range(len(y_train)):\n",
    "                if y_pred[i] != y_train[i]:\n",
    "                    self.class_prototypes[y_train[i]] += lr * x_encoded[i]\n",
    "                    if subtract_wrong: self.class_prototypes[1-y_train[i]] -= lr * x_encoded[i]\n",
    "\n",
    "    def train_multicentroid(self, x_encoded, y_train, threshold=0.25, reduce=True):\n",
    "        prototypes, proto_labels = [], []\n",
    "        for i in range(len(y_train)):\n",
    "            sample_hv, correct_label = x_encoded[i], y_train[i]; best_sim, best_proto_idx = -1, -1\n",
    "            if prototypes:\n",
    "                similarities = cosine_similarity(sample_hv.reshape(1, -1), np.array(prototypes))[0]\n",
    "                for j, label in enumerate(proto_labels):\n",
    "                    if label == correct_label and similarities[j] > best_sim: \n",
    "                        best_sim, best_proto_idx = similarities[j], j\n",
    "            if best_sim < threshold: \n",
    "                prototypes.append(sample_hv); proto_labels.append(correct_label)\n",
    "            else: \n",
    "                prototypes[best_proto_idx] += sample_hv\n",
    "        final_prototypes = np.zeros((self.num_classes, self.D))\n",
    "        for label in range(self.num_classes):\n",
    "            indices = [i for i, l in enumerate(proto_labels) if l == label]\n",
    "            if indices: final_prototypes[label] = np.sum(np.array(prototypes)[indices], axis=0)\n",
    "        self.class_prototypes = final_prototypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33194d8",
   "metadata": {},
   "source": [
    "Função de Pós-Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5a01c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_predictions(predictions, window_size=5, merge_gap=30, fs=0.5):\n",
    "    smoothed_preds = np.copy(predictions)\n",
    "    for i in range(len(predictions)):\n",
    "        start, end = max(0, i - window_size // 2), min(len(predictions), i + window_size // 2 + 1)\n",
    "        if np.mean(predictions[start:end]) < 0.5: smoothed_preds[i] = 0\n",
    "    return smoothed_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb57a96",
   "metadata": {},
   "source": [
    "**Parâmetros para a entrega da Semana 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb019f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serviço do Google Drive conectado com sucesso.\n",
      "Caminho do dataset encontrado no Drive!\n",
      "Arquivo salvo em: ./temp_summary.txt\n",
      "\n",
      "--- Processando 10 arquivos ---\n",
      "Processando: chb01_01.edf...\n",
      "Arquivo salvo em: ./temp_chb01_01.edf\n",
      "Processando: chb01_02.edf...\n",
      "Arquivo salvo em: ./temp_chb01_02.edf\n",
      "Processando: chb01_03.edf...\n",
      "Arquivo salvo em: ./temp_chb01_03.edf\n",
      "Processando: chb01_04.edf...\n",
      "Arquivo salvo em: ./temp_chb01_04.edf\n",
      "Processando: chb01_05.edf...\n",
      "Arquivo salvo em: ./temp_chb01_05.edf\n",
      "Processando: chb01_06.edf...\n",
      "Arquivo salvo em: ./temp_chb01_06.edf\n",
      "Processando: chb01_07.edf...\n",
      "Arquivo salvo em: ./temp_chb01_07.edf\n",
      "Processando: chb01_08.edf...\n",
      "Arquivo salvo em: ./temp_chb01_08.edf\n",
      "Processando: chb01_09.edf...\n",
      "Arquivo salvo em: ./temp_chb01_09.edf\n",
      "Processando: chb01_10.edf...\n",
      "Arquivo salvo em: ./temp_chb01_10.edf\n",
      "Codificação concluída.\n",
      "\n",
      "--- TREINANDO E AVALIANDO ESTRATÉGIA: Padrão ---\n",
      "\n",
      "--- TREINANDO E AVALIANDO ESTRATÉGIA: Multi-Pass ---\n",
      "Erro: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "\n",
      "Tempo total: 38.19 min\n",
      "\n",
      "\n",
      "--- RESULTADOS PACIENTE: chb01 ---\n",
      "\n",
      "Estratégia: Padrão\n",
      "  F1: 0.8421, Precisão: 0.9412, Sensibilidade: 0.7619\n",
      "  Matriz de Confusão:\n",
      "    VN: 20 | FP: 1\n",
      "    FN: 5 | VP: 16\n"
     ]
    }
   ],
   "source": [
    "PATIENT_ID = 'chb01'\n",
    "DRIVE_PATH_COMPONENTS = ['TCC EPILEPSIA DATA', 'chb-mit-scalp-eeg-database-1.0.0']\n",
    "\n",
    "MAX_GRAVACOES_POR_PACIENTE = 10\n",
    "DIMENSIONS = 10000\n",
    "NUM_LEVELS = 100\n",
    "EPOCHS_MULTIPASS = 12          \n",
    "LEARNING_RATE_MULTIPASS = 0.1\n",
    "THRESHOLD_MULTICENTROID = 0.25\n",
    "\n",
    "SMOOTHING_WINDOW_SIZE = 5      \n",
    "MERGE_SEIZURES_THRESHOLD = 30  \n",
    "MAX_CHANNELS = None              \n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    service = get_drive_service()\n",
    "    main_folder_id = find_folder_id_by_path(service, DRIVE_PATH_COMPONENTS)\n",
    "    patient_folder_id = find_folder_id(service, PATIENT_ID, parent_id=main_folder_id)\n",
    "    drive_files = get_files_from_drive_folder(service, patient_folder_id)\n",
    "\n",
    "    summary_filename = f\"{PATIENT_ID}-summary.txt\"\n",
    "    local_summary_path = \"./temp_summary.txt\"\n",
    "    download_file_locally(service, drive_files[summary_filename], local_summary_path)\n",
    "    seizure_times = parse_summary_file(local_summary_path)\n",
    "    os.remove(local_summary_path)\n",
    "\n",
    "    edf_files = sorted([n for n in drive_files.keys() if n.endswith('.edf')])[:MAX_GRAVACOES_POR_PACIENTE]\n",
    "\n",
    "    all_features, all_labels = [], []\n",
    "    print(f\"\\n--- Processando {len(edf_files)} arquivos ---\")\n",
    "    for edf_file in edf_files:\n",
    "        local_temp_path = f\"./temp_{edf_file}\"\n",
    "        print(f\"Processando: {edf_file}...\")\n",
    "        try:\n",
    "            download_file_locally(service, drive_files[edf_file], local_temp_path)\n",
    "            with pyedflib.EdfReader(local_temp_path) as f:\n",
    "                fs = f.getSampleFrequency(0)\n",
    "\n",
    "                if MAX_CHANNELS is None:\n",
    "                    n_channels = f.signals_in_file\n",
    "                else:\n",
    "                    n_channels = min(int(MAX_CHANNELS), int(f.signals_in_file))\n",
    "\n",
    "                signals = np.array([f.readSignal(c) for c in range(n_channels)])\n",
    "\n",
    "                window_samples = int(fs * 2)\n",
    "                step = max(1, window_samples // 2)\n",
    "\n",
    "                total = signals.shape[1]\n",
    "                if total < window_samples:\n",
    "                    n_windows = 0\n",
    "                else:\n",
    "                    n_windows = 1 + (total - window_samples) // step\n",
    "\n",
    "                for j in range(n_windows):\n",
    "                    start = j * step\n",
    "                    end = start + window_samples\n",
    "                    window = signals[:, start:end]\n",
    "\n",
    "                    is_seizure = any(\n",
    "                        max(start/fs, s_start) < min(end/fs, s_end)\n",
    "                        for s_start, s_end in seizure_times.get(edf_file, [])\n",
    "                    )\n",
    "\n",
    "                    feats = np.mean(\n",
    "                        [extract_single_feature_vector(window[c, :], fs) for c in range(n_channels)],\n",
    "                        axis=0\n",
    "                    )\n",
    "                    all_features.append(feats)\n",
    "                    all_labels.append(1 if is_seizure else 0)\n",
    "        finally:\n",
    "            if os.path.exists(local_temp_path):\n",
    "                os.remove(local_temp_path)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    X, y = np.array(all_features), np.array(all_labels)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    X_seiz, X_non = X_scaled[y==1], X_scaled[y==0]\n",
    "    n_non = min(len(X_non), len(X_seiz))\n",
    "    X_non_bal = resample(X_non, replace=False, n_samples=n_non, random_state=42)\n",
    "\n",
    "    X_bal = np.vstack((X_seiz, X_non_bal))\n",
    "    y_bal = np.hstack((np.ones(len(X_seiz)), np.zeros(n_non)))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_bal, y_bal, test_size=0.3, random_state=42, stratify=y_bal\n",
    "    )\n",
    "\n",
    "    NUM_FEATURES = X_train.shape[1]\n",
    "    hdc_encoder = HDC(DIMENSIONS, NUM_FEATURES, NUM_LEVELS, seed=42)\n",
    "    X_train_hd = hdc_encoder.encode(X_train)\n",
    "    X_test_hd = hdc_encoder.encode(X_test)\n",
    "    print(\"Codificação concluída.\")\n",
    "\n",
    "    strategies = ['Padrão', 'Multi-Pass', 'Multi-Centroid', 'MC+MP']\n",
    "    results = {}\n",
    "\n",
    "    for strat in strategies:\n",
    "        print(f\"\\n--- TREINANDO E AVALIANDO ESTRATÉGIA: {strat} ---\")\n",
    "        hdc_model = HDC(DIMENSIONS, NUM_FEATURES, NUM_LEVELS, seed=42)\n",
    "\n",
    "        if strat == 'Padrão':\n",
    "            hdc_model.train_standard(X_train_hd, y_train)\n",
    "        elif strat == 'Multi-Pass':\n",
    "            hdc_model.train_multipass(X_train_hd, y_train,\n",
    "                                      epochs=EPOCHS_MULTIPASS,\n",
    "                                      lr=LEARNING_RATE_MULTIPASS,\n",
    "                                      subtract_wrong=True)\n",
    "        elif strat == 'Multi-Centroid':\n",
    "            hdc_model.train_multicentroid(X_train_hd, y_train,\n",
    "                                          threshold=THRESHOLD_MULTICENTROID,\n",
    "                                          reduce=True)\n",
    "        elif strat == 'MC+MP':\n",
    "            hdc_model.train_multicentroid(X_train_hd, y_train,\n",
    "                                          threshold=THRESHOLD_MULTICENTROID,\n",
    "                                          reduce=True)\n",
    "            hdc_model.train_multipass(X_train_hd, y_train,\n",
    "                                      epochs=EPOCHS_MULTIPASS,\n",
    "                                      lr=LEARNING_RATE_MULTIPASS,\n",
    "                                      subtract_wrong=True,\n",
    "                                      initial_training=False)\n",
    "\n",
    "        preds = hdc_model.predict(X_test_hd)\n",
    "        results[strat] = {'Predições': preds}\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro: {e}\")\n",
    "finally:\n",
    "    print(f\"\\nTempo total: {(time.time() - start_time)/60:.2f} min\", flush=True)\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    print(f\"\\n\\n--- RESULTADOS PACIENTE: {PATIENT_ID} ---\")\n",
    "    for strat, data in results.items():\n",
    "        y_pred_smoothed = post_process_predictions(\n",
    "            data['Predições'],\n",
    "            window_size=SMOOTHING_WINDOW_SIZE,\n",
    "            merge_gap=MERGE_SEIZURES_THRESHOLD,\n",
    "            fs=0.5\n",
    "        )\n",
    "        cm = confusion_matrix(y_test, y_pred_smoothed, labels=[0, 1])\n",
    "        print(f\"\\nEstratégia: {strat}\")\n",
    "        print(f\"  F1: {f1_score(y_test, y_pred_smoothed, zero_division=0):.4f}, \"\n",
    "              f\"Precisão: {precision_score(y_test, y_pred_smoothed, zero_division=0):.4f}, \"\n",
    "              f\"Sensibilidade: {recall_score(y_test, y_pred_smoothed, zero_division=0):.4f}\")\n",
    "        print(\"  Matriz de Confusão:\")\n",
    "        print(f\"    VN: {cm[0][0]} | FP: {cm[0][1]}\")\n",
    "        print(f\"    FN: {cm[1][0]} | VP: {cm[1][1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbc1e9a",
   "metadata": {},
   "source": [
    "38 minutos e 10 segundos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffc331bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serviço do Google Drive conectado com sucesso.\n",
      "Caminho do dataset encontrado no Drive!\n",
      "Arquivo salvo em: ./temp_summary.txt\n",
      "\n",
      "--- Processando 10 arquivos ---\n",
      "Processando: chb01_01.edf...\n",
      "Arquivo salvo em: ./temp_chb01_01.edf\n",
      "Processando: chb01_02.edf...\n",
      "Arquivo salvo em: ./temp_chb01_02.edf\n",
      "Processando: chb01_03.edf...\n",
      "Arquivo salvo em: ./temp_chb01_03.edf\n",
      "Processando: chb01_04.edf...\n",
      "Arquivo salvo em: ./temp_chb01_04.edf\n",
      "Processando: chb01_05.edf...\n",
      "Arquivo salvo em: ./temp_chb01_05.edf\n",
      "Processando: chb01_06.edf...\n",
      "Arquivo salvo em: ./temp_chb01_06.edf\n",
      "Processando: chb01_07.edf...\n",
      "Arquivo salvo em: ./temp_chb01_07.edf\n",
      "Processando: chb01_08.edf...\n",
      "Arquivo salvo em: ./temp_chb01_08.edf\n",
      "Processando: chb01_09.edf...\n",
      "Arquivo salvo em: ./temp_chb01_09.edf\n",
      "Processando: chb01_10.edf...\n",
      "Arquivo salvo em: ./temp_chb01_10.edf\n",
      "Codificação concluída.\n",
      "\n",
      "--- TREINANDO E AVALIANDO ESTRATÉGIA: Padrão ---\n",
      "\n",
      "--- TREINANDO E AVALIANDO ESTRATÉGIA: Multi-Pass ---\n",
      "Erro: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "\n",
      "Tempo total: 33.07 min\n",
      "\n",
      "\n",
      "--- RESULTADOS PACIENTE: chb01 ---\n",
      "\n",
      "Estratégia: Padrão\n",
      "  F1: 0.7222, Precisão: 0.8667, Sensibilidade: 0.6190\n",
      "  Matriz de Confusão:\n",
      "    VN: 19 | FP: 2\n",
      "    FN: 8 | VP: 13\n"
     ]
    }
   ],
   "source": [
    "PATIENT_ID = 'chb01'\n",
    "DRIVE_PATH_COMPONENTS = ['TCC EPILEPSIA DATA', 'chb-mit-scalp-eeg-database-1.0.0']\n",
    "\n",
    "MAX_GRAVACOES_POR_PACIENTE = 10\n",
    "DIMENSIONS = 10000\n",
    "NUM_LEVELS = 100\n",
    "EPOCHS_MULTIPASS = 20          \n",
    "LEARNING_RATE_MULTIPASS = 0.1\n",
    "THRESHOLD_MULTICENTROID = 0.25\n",
    "\n",
    "SMOOTHING_WINDOW_SIZE = 7      \n",
    "MERGE_SEIZURES_THRESHOLD = 30  \n",
    "MAX_CHANNELS = 16              \n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    service = get_drive_service()\n",
    "    main_folder_id = find_folder_id_by_path(service, DRIVE_PATH_COMPONENTS)\n",
    "    patient_folder_id = find_folder_id(service, PATIENT_ID, parent_id=main_folder_id)\n",
    "    drive_files = get_files_from_drive_folder(service, patient_folder_id)\n",
    "\n",
    "    summary_filename = f\"{PATIENT_ID}-summary.txt\"\n",
    "    local_summary_path = \"./temp_summary.txt\"\n",
    "    download_file_locally(service, drive_files[summary_filename], local_summary_path)\n",
    "    seizure_times = parse_summary_file(local_summary_path)\n",
    "    os.remove(local_summary_path)\n",
    "\n",
    "    edf_files = sorted([n for n in drive_files.keys() if n.endswith('.edf')])[:MAX_GRAVACOES_POR_PACIENTE]\n",
    "\n",
    "    all_features, all_labels = [], []\n",
    "    print(f\"\\n--- Processando {len(edf_files)} arquivos ---\")\n",
    "    for edf_file in edf_files:\n",
    "        local_temp_path = f\"./temp_{edf_file}\"\n",
    "        print(f\"Processando: {edf_file}...\")\n",
    "        try:\n",
    "            download_file_locally(service, drive_files[edf_file], local_temp_path)\n",
    "            with pyedflib.EdfReader(local_temp_path) as f:\n",
    "                fs = f.getSampleFrequency(0)\n",
    "\n",
    "                if MAX_CHANNELS is None:\n",
    "                    n_channels = f.signals_in_file\n",
    "                else:\n",
    "                    n_channels = min(int(MAX_CHANNELS), int(f.signals_in_file))\n",
    "\n",
    "                signals = np.array([f.readSignal(c) for c in range(n_channels)])\n",
    "\n",
    "                window_samples = int(fs * 2)\n",
    "                step = max(1, window_samples // 2)\n",
    "\n",
    "                total = signals.shape[1]\n",
    "                if total < window_samples:\n",
    "                    n_windows = 0\n",
    "                else:\n",
    "                    n_windows = 1 + (total - window_samples) // step\n",
    "\n",
    "                for j in range(n_windows):\n",
    "                    start = j * step\n",
    "                    end = start + window_samples\n",
    "                    window = signals[:, start:end]\n",
    "\n",
    "                    is_seizure = any(\n",
    "                        max(start/fs, s_start) < min(end/fs, s_end)\n",
    "                        for s_start, s_end in seizure_times.get(edf_file, [])\n",
    "                    )\n",
    "\n",
    "                    feats = np.mean(\n",
    "                        [extract_single_feature_vector(window[c, :], fs) for c in range(n_channels)],\n",
    "                        axis=0\n",
    "                    )\n",
    "                    all_features.append(feats)\n",
    "                    all_labels.append(1 if is_seizure else 0)\n",
    "        finally:\n",
    "            if os.path.exists(local_temp_path):\n",
    "                os.remove(local_temp_path)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    X, y = np.array(all_features), np.array(all_labels)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    X_seiz, X_non = X_scaled[y==1], X_scaled[y==0]\n",
    "    n_non = min(len(X_non), len(X_seiz))\n",
    "    X_non_bal = resample(X_non, replace=False, n_samples=n_non, random_state=42)\n",
    "\n",
    "    X_bal = np.vstack((X_seiz, X_non_bal))\n",
    "    y_bal = np.hstack((np.ones(len(X_seiz)), np.zeros(n_non)))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_bal, y_bal, test_size=0.3, random_state=42, stratify=y_bal\n",
    "    )\n",
    "\n",
    "    NUM_FEATURES = X_train.shape[1]\n",
    "    hdc_encoder = HDC(DIMENSIONS, NUM_FEATURES, NUM_LEVELS, seed=42)\n",
    "    X_train_hd = hdc_encoder.encode(X_train)\n",
    "    X_test_hd = hdc_encoder.encode(X_test)\n",
    "    print(\"Codificação concluída.\")\n",
    "\n",
    "    strategies = ['Padrão', 'Multi-Pass', 'Multi-Centroid', 'MC+MP']\n",
    "    results = {}\n",
    "\n",
    "    for strat in strategies:\n",
    "        print(f\"\\n--- TREINANDO E AVALIANDO ESTRATÉGIA: {strat} ---\")\n",
    "        hdc_model = HDC(DIMENSIONS, NUM_FEATURES, NUM_LEVELS, seed=42)\n",
    "\n",
    "        if strat == 'Padrão':\n",
    "            hdc_model.train_standard(X_train_hd, y_train)\n",
    "        elif strat == 'Multi-Pass':\n",
    "            hdc_model.train_multipass(X_train_hd, y_train,\n",
    "                                      epochs=EPOCHS_MULTIPASS,\n",
    "                                      lr=LEARNING_RATE_MULTIPASS,\n",
    "                                      subtract_wrong=True)\n",
    "        elif strat == 'Multi-Centroid':\n",
    "            hdc_model.train_multicentroid(X_train_hd, y_train,\n",
    "                                          threshold=THRESHOLD_MULTICENTROID,\n",
    "                                          reduce=True)\n",
    "        elif strat == 'MC+MP':\n",
    "            hdc_model.train_multicentroid(X_train_hd, y_train,\n",
    "                                          threshold=THRESHOLD_MULTICENTROID,\n",
    "                                          reduce=True)\n",
    "            hdc_model.train_multipass(X_train_hd, y_train,\n",
    "                                      epochs=EPOCHS_MULTIPASS,\n",
    "                                      lr=LEARNING_RATE_MULTIPASS,\n",
    "                                      subtract_wrong=True,\n",
    "                                      initial_training=False)\n",
    "\n",
    "        preds = hdc_model.predict(X_test_hd)\n",
    "        results[strat] = {'Predições': preds}\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro: {e}\")\n",
    "finally:\n",
    "    print(f\"\\nTempo total: {(time.time() - start_time)/60:.2f} min\", flush=True)\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    print(f\"\\n\\n--- RESULTADOS PACIENTE: {PATIENT_ID} ---\")\n",
    "    for strat, data in results.items():\n",
    "        y_pred_smoothed = post_process_predictions(\n",
    "            data['Predições'],\n",
    "            window_size=SMOOTHING_WINDOW_SIZE,\n",
    "            merge_gap=MERGE_SEIZURES_THRESHOLD,\n",
    "            fs=0.5\n",
    "        )\n",
    "        cm = confusion_matrix(y_test, y_pred_smoothed, labels=[0, 1])\n",
    "        print(f\"\\nEstratégia: {strat}\")\n",
    "        print(f\"  F1: {f1_score(y_test, y_pred_smoothed, zero_division=0):.4f}, \"\n",
    "              f\"Precisão: {precision_score(y_test, y_pred_smoothed, zero_division=0):.4f}, \"\n",
    "              f\"Sensibilidade: {recall_score(y_test, y_pred_smoothed, zero_division=0):.4f}\")\n",
    "        print(\"  Matriz de Confusão:\")\n",
    "        print(f\"    VN: {cm[0][0]} | FP: {cm[0][1]}\")\n",
    "        print(f\"    FN: {cm[1][0]} | VP: {cm[1][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1238aa",
   "metadata": {},
   "source": [
    "33 minutos e 5 segundos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8254761b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serviço do Google Drive conectado com sucesso.\n",
      "Caminho do dataset encontrado no Drive!\n",
      "Arquivo salvo em: ./temp_summary.txt\n",
      "\n",
      "--- Processando 10 arquivos ---\n",
      "Processando: chb01_01.edf...\n",
      "Arquivo salvo em: ./temp_chb01_01.edf\n",
      "Processando: chb01_02.edf...\n",
      "Arquivo salvo em: ./temp_chb01_02.edf\n",
      "Processando: chb01_03.edf...\n",
      "Arquivo salvo em: ./temp_chb01_03.edf\n",
      "Processando: chb01_04.edf...\n",
      "Arquivo salvo em: ./temp_chb01_04.edf\n",
      "Processando: chb01_05.edf...\n",
      "Arquivo salvo em: ./temp_chb01_05.edf\n",
      "Processando: chb01_06.edf...\n",
      "Arquivo salvo em: ./temp_chb01_06.edf\n",
      "Processando: chb01_07.edf...\n",
      "Arquivo salvo em: ./temp_chb01_07.edf\n",
      "Processando: chb01_08.edf...\n",
      "Arquivo salvo em: ./temp_chb01_08.edf\n",
      "Processando: chb01_09.edf...\n",
      "Arquivo salvo em: ./temp_chb01_09.edf\n",
      "Processando: chb01_10.edf...\n",
      "Arquivo salvo em: ./temp_chb01_10.edf\n",
      "Codificação concluída.\n",
      "Erro: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "\n",
      "Tempo total: 41.27 min\n",
      "\n",
      "=== RESULTADOS COMPARATIVOS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estratégia</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precisão</th>\n",
       "      <th>Sensibilidade</th>\n",
       "      <th>VN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>VP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Padrão</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Estratégia        F1  Precisão  Sensibilidade  VN  FP  FN  VP\n",
       "0     Padrão  0.842105  0.941176       0.761905  20   1   5  16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PATIENT_ID = 'chb01'\n",
    "DRIVE_PATH_COMPONENTS = ['TCC EPILEPSIA DATA', 'chb-mit-scalp-eeg-database-1.0.0']\n",
    "\n",
    "MAX_GRAVACOES_POR_PACIENTE = 10\n",
    "DIMENSIONS = 10000\n",
    "NUM_LEVELS = 100\n",
    "EPOCHS_MULTIPASS = 12          \n",
    "LEARNING_RATE_MULTIPASS = 0.1\n",
    "THRESHOLD_MULTICENTROID = 0.25\n",
    "\n",
    "SMOOTHING_WINDOW_SIZE = 5      \n",
    "MERGE_SEIZURES_THRESHOLD = 30  \n",
    "MAX_CHANNELS = None              \n",
    "\n",
    "start_time = time.time()\n",
    "results = {}\n",
    "\n",
    "try:\n",
    "    service = get_drive_service()\n",
    "    main_folder_id = find_folder_id_by_path(service, DRIVE_PATH_COMPONENTS)\n",
    "    patient_folder_id = find_folder_id(service, PATIENT_ID, parent_id=main_folder_id)\n",
    "    drive_files = get_files_from_drive_folder(service, patient_folder_id)\n",
    "\n",
    "    summary_filename = f\"{PATIENT_ID}-summary.txt\"\n",
    "    local_summary_path = \"./temp_summary.txt\"\n",
    "    download_file_locally(service, drive_files[summary_filename], local_summary_path)\n",
    "    seizure_times = parse_summary_file(local_summary_path)\n",
    "    os.remove(local_summary_path)\n",
    "\n",
    "    edf_files = sorted([n for n in drive_files.keys() if n.endswith('.edf')])[:MAX_GRAVACOES_POR_PACIENTE]\n",
    "\n",
    "    all_features, all_labels = [], []\n",
    "    print(f\"\\n--- Processando {len(edf_files)} arquivos ---\")\n",
    "    for edf_file in edf_files:\n",
    "        local_temp_path = f\"./temp_{edf_file}\"\n",
    "        print(f\"Processando: {edf_file}...\")\n",
    "        try:\n",
    "            download_file_locally(service, drive_files[edf_file], local_temp_path)\n",
    "            with pyedflib.EdfReader(local_temp_path) as f:\n",
    "                fs = f.getSampleFrequency(0)\n",
    "\n",
    "                if MAX_CHANNELS is None:\n",
    "                    n_channels = f.signals_in_file\n",
    "                else:\n",
    "                    n_channels = min(int(MAX_CHANNELS), int(f.signals_in_file))\n",
    "\n",
    "                signals = np.array([f.readSignal(c) for c in range(n_channels)])\n",
    "\n",
    "                window_samples = int(fs * 2)\n",
    "                step = max(1, window_samples // 2)\n",
    "\n",
    "                total = signals.shape[1]\n",
    "                if total < window_samples:\n",
    "                    n_windows = 0\n",
    "                else:\n",
    "                    n_windows = 1 + (total - window_samples) // step\n",
    "\n",
    "                for j in range(n_windows):\n",
    "                    start = j * step\n",
    "                    end = start + window_samples\n",
    "                    window = signals[:, start:end]\n",
    "\n",
    "                    is_seizure = any(\n",
    "                        max(start/fs, s_start) < min(end/fs, s_end)\n",
    "                        for s_start, s_end in seizure_times.get(edf_file, [])\n",
    "                    )\n",
    "\n",
    "                    feats = np.mean(\n",
    "                        [extract_single_feature_vector(window[c, :], fs) for c in range(n_channels)],\n",
    "                        axis=0\n",
    "                    )\n",
    "                    all_features.append(feats)\n",
    "                    all_labels.append(1 if is_seizure else 0)\n",
    "        finally:\n",
    "            if os.path.exists(local_temp_path):\n",
    "                os.remove(local_temp_path)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    X, y = np.array(all_features), np.array(all_labels)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    X_seiz, X_non = X_scaled[y==1], X_scaled[y==0]\n",
    "    n_non = min(len(X_non), len(X_seiz))\n",
    "    X_non_bal = resample(X_non, replace=False, n_samples=n_non, random_state=42)\n",
    "\n",
    "    X_bal = np.vstack((X_seiz, X_non_bal))\n",
    "    y_bal = np.hstack((np.ones(len(X_seiz)), np.zeros(n_non)))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_bal, y_bal, test_size=0.3, random_state=42, stratify=y_bal\n",
    "    )\n",
    "\n",
    "    NUM_FEATURES = X_train.shape[1]\n",
    "    hdc_encoder = HDC(DIMENSIONS, NUM_FEATURES, NUM_LEVELS, seed=42) \n",
    "    X_train_hd = hdc_encoder.encode(X_train)\n",
    "    X_test_hd = hdc_encoder.encode(X_test)\n",
    "    print(\"Codificação concluída.\")\n",
    "\n",
    "    strategies = ['Padrão', 'Multi-Pass', 'Multi-Centroid', 'MC+MP']\n",
    "\n",
    "    for strat in strategies:\n",
    "        hdc_model = HDC(DIMENSIONS, NUM_FEATURES, NUM_LEVELS, seed=42)  \n",
    "        if strat == 'Padrão':\n",
    "            hdc_model.train_standard(X_train_hd, y_train)\n",
    "        elif strat == 'Multi-Pass':\n",
    "            hdc_model.train_multipass(X_train_hd, y_train,\n",
    "                                      epochs=EPOCHS_MULTIPASS,\n",
    "                                      lr=LEARNING_RATE_MULTIPASS,\n",
    "                                      subtract_wrong=True)\n",
    "        elif strat == 'Multi-Centroid':\n",
    "            hdc_model.train_multicentroid(X_train_hd, y_train,\n",
    "                                          threshold=THRESHOLD_MULTICENTROID,\n",
    "                                          reduce=True)\n",
    "        elif strat == 'MC+MP':\n",
    "            hdc_model.train_multicentroid(X_train_hd, y_train,\n",
    "                                          threshold=THRESHOLD_MULTICENTROID,\n",
    "                                          reduce=True)\n",
    "            hdc_model.train_multipass(X_train_hd, y_train,\n",
    "                                      epochs=EPOCHS_MULTIPASS,\n",
    "                                      lr=LEARNING_RATE_MULTIPASS,\n",
    "                                      subtract_wrong=True,\n",
    "                                      initial_training=False)\n",
    "\n",
    "        preds = hdc_model.predict(X_test_hd)\n",
    "        results[strat] = {'Predições': preds}\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro: {e}\")\n",
    "finally:\n",
    "    print(f\"\\nTempo total: {(time.time() - start_time)/60:.2f} min\", flush=True)\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    all_metrics = []\n",
    "    for strat, data in results.items():\n",
    "        y_pred_smoothed = post_process_predictions(\n",
    "            data['Predições'],\n",
    "            window_size=SMOOTHING_WINDOW_SIZE,\n",
    "            merge_gap=MERGE_SEIZURES_THRESHOLD,\n",
    "            fs=0.5\n",
    "        )\n",
    "        cm = confusion_matrix(y_test, y_pred_smoothed, labels=[0, 1])\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred_smoothed, zero_division=0)\n",
    "        prec = precision_score(y_test, y_pred_smoothed, zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred_smoothed, zero_division=0)\n",
    "\n",
    "        all_metrics.append({\n",
    "            \"Estratégia\": strat,\n",
    "            \"F1\": f1,\n",
    "            \"Precisão\": prec,\n",
    "            \"Sensibilidade\": rec,\n",
    "            \"VN\": cm[0][0],\n",
    "            \"FP\": cm[0][1],\n",
    "            \"FN\": cm[1][0],\n",
    "            \"VP\": cm[1][1]\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(all_metrics)\n",
    "    print(\"\\n=== RESULTADOS COMPARATIVOS ===\")\n",
    "    display(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e95e430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serviço do Google Drive conectado com sucesso.\n",
      "Caminho do dataset encontrado no Drive!\n",
      "Arquivo salvo em: ./temp_summary.txt\n",
      "\n",
      "--- Processando 10 arquivos ---\n",
      "Processando: chb01_01.edf...\n",
      "Arquivo salvo em: ./temp_chb01_01.edf\n",
      "Processando: chb01_02.edf...\n",
      "Arquivo salvo em: ./temp_chb01_02.edf\n",
      "Processando: chb01_03.edf...\n",
      "Arquivo salvo em: ./temp_chb01_03.edf\n",
      "Processando: chb01_04.edf...\n",
      "Arquivo salvo em: ./temp_chb01_04.edf\n",
      "Processando: chb01_05.edf...\n",
      "Arquivo salvo em: ./temp_chb01_05.edf\n",
      "Processando: chb01_06.edf...\n",
      "Arquivo salvo em: ./temp_chb01_06.edf\n",
      "Processando: chb01_07.edf...\n",
      "Arquivo salvo em: ./temp_chb01_07.edf\n",
      "Processando: chb01_08.edf...\n",
      "Arquivo salvo em: ./temp_chb01_08.edf\n",
      "Processando: chb01_09.edf...\n",
      "Arquivo salvo em: ./temp_chb01_09.edf\n",
      "Processando: chb01_10.edf...\n",
      "Arquivo salvo em: ./temp_chb01_10.edf\n",
      "Codificação concluída.\n",
      "Erro: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "\n",
      "Tempo total: 27.35 min\n",
      "\n",
      "=== RESULTADOS COMPARATIVOS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estratégia</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precisão</th>\n",
       "      <th>Sensibilidade</th>\n",
       "      <th>VN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>VP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Padrão</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Estratégia        F1  Precisão  Sensibilidade  VN  FP  FN  VP\n",
       "0     Padrão  0.722222  0.866667       0.619048  19   2   8  13"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PATIENT_ID = 'chb01'\n",
    "DRIVE_PATH_COMPONENTS = ['TCC EPILEPSIA DATA', 'chb-mit-scalp-eeg-database-1.0.0']\n",
    "\n",
    "MAX_GRAVACOES_POR_PACIENTE = 10\n",
    "DIMENSIONS = 10000\n",
    "NUM_LEVELS = 100\n",
    "EPOCHS_MULTIPASS = 20          \n",
    "LEARNING_RATE_MULTIPASS = 0.1\n",
    "THRESHOLD_MULTICENTROID = 0.25\n",
    "\n",
    "SMOOTHING_WINDOW_SIZE = 7      \n",
    "MERGE_SEIZURES_THRESHOLD = 30  \n",
    "MAX_CHANNELS = 16              \n",
    "\n",
    "start_time = time.time()\n",
    "results = {}\n",
    "\n",
    "try:\n",
    "    service = get_drive_service()\n",
    "    main_folder_id = find_folder_id_by_path(service, DRIVE_PATH_COMPONENTS)\n",
    "    patient_folder_id = find_folder_id(service, PATIENT_ID, parent_id=main_folder_id)\n",
    "    drive_files = get_files_from_drive_folder(service, patient_folder_id)\n",
    "\n",
    "    summary_filename = f\"{PATIENT_ID}-summary.txt\"\n",
    "    local_summary_path = \"./temp_summary.txt\"\n",
    "    download_file_locally(service, drive_files[summary_filename], local_summary_path)\n",
    "    seizure_times = parse_summary_file(local_summary_path)\n",
    "    os.remove(local_summary_path)\n",
    "\n",
    "    edf_files = sorted([n for n in drive_files.keys() if n.endswith('.edf')])[:MAX_GRAVACOES_POR_PACIENTE]\n",
    "\n",
    "    all_features, all_labels = [], []\n",
    "    print(f\"\\n--- Processando {len(edf_files)} arquivos ---\")\n",
    "    for edf_file in edf_files:\n",
    "        local_temp_path = f\"./temp_{edf_file}\"\n",
    "        print(f\"Processando: {edf_file}...\")\n",
    "        try:\n",
    "            download_file_locally(service, drive_files[edf_file], local_temp_path)\n",
    "            with pyedflib.EdfReader(local_temp_path) as f:\n",
    "                fs = f.getSampleFrequency(0)\n",
    "\n",
    "                if MAX_CHANNELS is None:\n",
    "                    n_channels = f.signals_in_file\n",
    "                else:\n",
    "                    n_channels = min(int(MAX_CHANNELS), int(f.signals_in_file))\n",
    "\n",
    "                signals = np.array([f.readSignal(c) for c in range(n_channels)])\n",
    "\n",
    "                window_samples = int(fs * 2)\n",
    "                step = max(1, window_samples // 2)\n",
    "\n",
    "                total = signals.shape[1]\n",
    "                if total < window_samples:\n",
    "                    n_windows = 0\n",
    "                else:\n",
    "                    n_windows = 1 + (total - window_samples) // step\n",
    "\n",
    "                for j in range(n_windows):\n",
    "                    start = j * step\n",
    "                    end = start + window_samples\n",
    "                    window = signals[:, start:end]\n",
    "\n",
    "                    is_seizure = any(\n",
    "                        max(start/fs, s_start) < min(end/fs, s_end)\n",
    "                        for s_start, s_end in seizure_times.get(edf_file, [])\n",
    "                    )\n",
    "\n",
    "                    feats = np.mean(\n",
    "                        [extract_single_feature_vector(window[c, :], fs) for c in range(n_channels)],\n",
    "                        axis=0\n",
    "                    )\n",
    "                    all_features.append(feats)\n",
    "                    all_labels.append(1 if is_seizure else 0)\n",
    "        finally:\n",
    "            if os.path.exists(local_temp_path):\n",
    "                os.remove(local_temp_path)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    X, y = np.array(all_features), np.array(all_labels)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    X_seiz, X_non = X_scaled[y==1], X_scaled[y==0]\n",
    "    n_non = min(len(X_non), len(X_seiz))\n",
    "    X_non_bal = resample(X_non, replace=False, n_samples=n_non, random_state=42)\n",
    "\n",
    "    X_bal = np.vstack((X_seiz, X_non_bal))\n",
    "    y_bal = np.hstack((np.ones(len(X_seiz)), np.zeros(n_non)))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_bal, y_bal, test_size=0.3, random_state=42, stratify=y_bal\n",
    "    )\n",
    "\n",
    "    NUM_FEATURES = X_train.shape[1]\n",
    "    hdc_encoder = HDC(DIMENSIONS, NUM_FEATURES, NUM_LEVELS, seed=42) \n",
    "    X_train_hd = hdc_encoder.encode(X_train)\n",
    "    X_test_hd = hdc_encoder.encode(X_test)\n",
    "    print(\"Codificação concluída.\")\n",
    "\n",
    "    strategies = ['Padrão', 'Multi-Pass', 'Multi-Centroid', 'MC+MP']\n",
    "\n",
    "    for strat in strategies:\n",
    "        hdc_model = HDC(DIMENSIONS, NUM_FEATURES, NUM_LEVELS, seed=42)  \n",
    "        if strat == 'Padrão':\n",
    "            hdc_model.train_standard(X_train_hd, y_train)\n",
    "        elif strat == 'Multi-Pass':\n",
    "            hdc_model.train_multipass(X_train_hd, y_train,\n",
    "                                      epochs=EPOCHS_MULTIPASS,\n",
    "                                      lr=LEARNING_RATE_MULTIPASS,\n",
    "                                      subtract_wrong=True)\n",
    "        elif strat == 'Multi-Centroid':\n",
    "            hdc_model.train_multicentroid(X_train_hd, y_train,\n",
    "                                          threshold=THRESHOLD_MULTICENTROID,\n",
    "                                          reduce=True)\n",
    "        elif strat == 'MC+MP':\n",
    "            hdc_model.train_multicentroid(X_train_hd, y_train,\n",
    "                                          threshold=THRESHOLD_MULTICENTROID,\n",
    "                                          reduce=True)\n",
    "            hdc_model.train_multipass(X_train_hd, y_train,\n",
    "                                      epochs=EPOCHS_MULTIPASS,\n",
    "                                      lr=LEARNING_RATE_MULTIPASS,\n",
    "                                      subtract_wrong=True,\n",
    "                                      initial_training=False)\n",
    "\n",
    "        preds = hdc_model.predict(X_test_hd)\n",
    "        results[strat] = {'Predições': preds}\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro: {e}\")\n",
    "finally:\n",
    "    print(f\"\\nTempo total: {(time.time() - start_time)/60:.2f} min\", flush=True)\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    all_metrics = []\n",
    "    for strat, data in results.items():\n",
    "        y_pred_smoothed = post_process_predictions(\n",
    "            data['Predições'],\n",
    "            window_size=SMOOTHING_WINDOW_SIZE,\n",
    "            merge_gap=MERGE_SEIZURES_THRESHOLD,\n",
    "            fs=0.5\n",
    "        )\n",
    "        cm = confusion_matrix(y_test, y_pred_smoothed, labels=[0, 1])\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred_smoothed, zero_division=0)\n",
    "        prec = precision_score(y_test, y_pred_smoothed, zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred_smoothed, zero_division=0)\n",
    "\n",
    "        all_metrics.append({\n",
    "            \"Estratégia\": strat,\n",
    "            \"F1\": f1,\n",
    "            \"Precisão\": prec,\n",
    "            \"Sensibilidade\": rec,\n",
    "            \"VN\": cm[0][0],\n",
    "            \"FP\": cm[0][1],\n",
    "            \"FN\": cm[1][0],\n",
    "            \"VP\": cm[1][1]\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(all_metrics)\n",
    "    print(\"\\n=== RESULTADOS COMPARATIVOS ===\")\n",
    "    display(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
