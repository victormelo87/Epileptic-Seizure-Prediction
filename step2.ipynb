{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e9137f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import io\n",
    "import gc\n",
    "import warnings\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import skew, kurtosis\n",
    "import antropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.utils import resample\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "import pyedflib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.readonly']\n",
    "\n",
    "def get_drive_service():\n",
    "    creds = None\n",
    "    creds_folder = 'credentials'\n",
    "    token_path = os.path.join(creds_folder, 'token.json')\n",
    "    credentials_path = os.path.join(creds_folder, 'credentials.json')\n",
    "\n",
    "    os.makedirs(creds_folder, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(token_path):\n",
    "        creds = Credentials.from_authorized_user_file(token_path, SCOPES)\n",
    "\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            if not os.path.exists(credentials_path):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"ERRO CRÍTICO: O arquivo 'credentials.json' não foi encontrado dentro da pasta '{creds_folder}'.\"\n",
    "                )\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(credentials_path, SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "\n",
    "        with open(token_path, 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    try:\n",
    "        service = build('drive', 'v3', credentials=creds)\n",
    "        print(\"Serviço do Google Drive conectado com sucesso.\")\n",
    "        return service\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao construir o serviço do Drive: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_folder_id(service, folder_name, parent_id='root'):\n",
    "    query = f\"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and '{parent_id}' in parents\"\n",
    "    results = service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
    "    items = results.get('files', [])\n",
    "    return items[0]['id'] if items else None\n",
    "\n",
    "\n",
    "def find_folder_id_by_path(service, path_components):\n",
    "    current_parent_id = 'root'\n",
    "    for folder_name in path_components:\n",
    "        query = f\"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and '{current_parent_id}' in parents\"\n",
    "        results = service.files().list(q=query, fields=\"files(id)\").execute()\n",
    "        items = results.get('files', [])\n",
    "        if not items:\n",
    "            print(f\"Pasta '{folder_name}' não encontrada em '{current_parent_id}'.\")\n",
    "            return None\n",
    "        current_parent_id = items[0]['id']\n",
    "    print(\"Caminho do dataset encontrado no Drive!\")\n",
    "    return current_parent_id\n",
    "\n",
    "\n",
    "def get_files_from_drive_folder(service, folder_id):\n",
    "    query = f\"'{folder_id}' in parents\"\n",
    "    results = service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
    "    return {file['name']: file['id'] for file in results.get('files', [])}\n",
    "\n",
    "\n",
    "def download_file_locally(service, file_id, local_filename):\n",
    "    request = service.files().get_media(fileId=file_id)\n",
    "    with io.FileIO(local_filename, 'wb') as fh:\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        done = False\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "            if status:\n",
    "                print(f\"Download {int(status.progress() * 100)}% concluído...\", end=\"\\r\")\n",
    "    print(f\"Arquivo salvo em: {local_filename}\")\n",
    "\n",
    "def parse_summary_file(file_path):\n",
    "    seizure_info = {}\n",
    "    with open(file_path, 'r', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    file_blocks = re.split(r'File Name:\\s*', content)\n",
    "\n",
    "    for block in file_blocks:\n",
    "        if not block.strip():\n",
    "            continue\n",
    "        \n",
    "        lines = block.strip().split('\\n')\n",
    "        file_name = lines[0].strip()\n",
    "        seizure_info[file_name] = []\n",
    "\n",
    "        start_time_pattern = re.compile(r\"Seizure\\s*\\d*\\s*Start Time:\\s*(\\d+)\\s*seconds\")\n",
    "        end_time_pattern = re.compile(r\"Seizure\\s*\\d*\\s*End Time:\\s*(\\d+)\\s*seconds\")\n",
    "\n",
    "        for i in range(len(lines)):\n",
    "            start_match = start_time_pattern.search(lines[i])\n",
    "            if start_match:\n",
    "                start_time = int(start_match.group(1))\n",
    "                if i + 1 < len(lines):\n",
    "                    end_match = end_time_pattern.search(lines[i+1])\n",
    "                    if end_match:\n",
    "                        end_time = int(end_match.group(1))\n",
    "                        seizure_info[file_name].append((start_time, end_time))\n",
    "    return seizure_info\n",
    "\n",
    "\n",
    "def extract_single_feature_vector(eeg_window, fs=256):\n",
    "    freqs, psd = welch(eeg_window, fs=fs, nperseg=len(eeg_window)); total_power = np.sum(psd)\n",
    "    def get_band_power(f_low, f_high): return np.sum(psd[np.logical_and(freqs >= f_low, freqs <= f_high)])\n",
    "    delta, theta, alpha, beta, gamma = get_band_power(0.5, 4), get_band_power(4, 8), get_band_power(8, 13), get_band_power(13, 30), get_band_power(30, 80)\n",
    "    band_powers = [p / total_power if total_power > 0 else 0 for p in [delta, theta, alpha, beta, gamma]]\n",
    "    ratios = [beta / alpha if alpha > 0 else 0, (delta + theta) / (alpha + beta) if (alpha + beta) > 0 else 0]\n",
    "    entropies = [antropy.perm_entropy(eeg_window, normalize=True), antropy.spectral_entropy(eeg_window, sf=fs, method='welch', normalize=True), antropy.sample_entropy(eeg_window)]\n",
    "    stats = [np.mean(np.abs(eeg_window)), np.std(eeg_window), skew(eeg_window), kurtosis(eeg_window)]\n",
    "    features = [total_power] + band_powers + ratios + entropies + stats\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if len(features) < 46:\n",
    "        padding = np.zeros(46 - len(features)); features = np.concatenate((features, padding))\n",
    "    return np.array(features[:46])\n",
    "\n",
    "class HDC:\n",
    "    def __init__(self, dimensions, num_features, num_levels, num_classes=2, seed=None):\n",
    "        if seed is not None: np.random.seed(seed)\n",
    "        self.D, self.num_features, self.num_levels, self.num_classes = dimensions, num_features, num_levels, num_classes\n",
    "        self.level_vectors = np.random.choice([-1, 1], size=(num_levels, self.D)); self.feature_vectors = np.random.choice([-1, 1], size=(num_features, self.D))\n",
    "        self.class_prototypes = np.zeros((self.num_classes, self.D))\n",
    "    def _quantize(self, data, num_levels):\n",
    "        min_val, max_val = np.min(data), np.max(data)\n",
    "        if max_val == min_val:\n",
    "            return np.zeros_like(data, dtype=int)\n",
    "        \n",
    "        return np.round((data - min_val) / (max_val - min_val) * (num_levels - 1)).astype(int)\n",
    "    def encode(self, x_data):\n",
    "        num_samples, num_features = x_data.shape; x_quantized = np.array([self._quantize(x_data[:, i], self.num_levels) for i in range(num_features)]).T\n",
    "        encoded_data = np.zeros((num_samples, self.D))\n",
    "        for i in range(num_samples):\n",
    "            sample_hv = np.sum([self.feature_vectors[f] * self.level_vectors[x_quantized[i, f]] for f in range(num_features)], axis=0)\n",
    "            encoded_data[i] = np.sign(sample_hv) if np.any(sample_hv) else np.zeros(self.D)\n",
    "        return encoded_data\n",
    "    def predict(self, x_encoded):\n",
    "        return np.argmax(cosine_similarity(x_encoded, self.class_prototypes), axis=1)\n",
    "    def train_standard(self, x_encoded, y_train):\n",
    "        self.class_prototypes = np.array([np.sum(x_encoded[y_train == i], axis=0) for i in range(self.num_classes)])\n",
    "    def train_multipass(self, x_encoded, y_train, epochs, lr, initial_training=True, subtract_wrong=False):\n",
    "        if initial_training: self.train_standard(x_encoded, y_train)\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.predict(x_encoded)\n",
    "            if np.sum(y_pred != y_train) == 0: break\n",
    "            for i in range(len(y_train)):\n",
    "                if y_pred[i] != y_train[i]:\n",
    "                    self.class_prototypes[y_train[i]] += lr * x_encoded[i]\n",
    "                    if subtract_wrong:\n",
    "                        self.class_prototypes[y_pred[i]] -= lr * x_encoded[i]\n",
    "    def train_multicentroid(self, x_encoded, y_train, threshold):\n",
    "        prototypes, proto_labels = [], []\n",
    "        for i in range(len(y_train)):\n",
    "            sample_hv, correct_label = x_encoded[i], y_train[i]; best_sim, best_proto_idx = -1, -1\n",
    "            if prototypes:\n",
    "                similarities = cosine_similarity(sample_hv.reshape(1, -1), np.array(prototypes))[0]\n",
    "                for j, label in enumerate(proto_labels):\n",
    "                    if label == correct_label and similarities[j] > best_sim: best_sim, best_proto_idx = similarities[j], j\n",
    "            if best_sim < threshold: prototypes.append(sample_hv); proto_labels.append(correct_label)\n",
    "            else: prototypes[best_proto_idx] += sample_hv\n",
    "        final_prototypes = np.zeros((self.num_classes, self.D))\n",
    "        for label in range(self.num_classes):\n",
    "            indices = [i for i, l in enumerate(proto_labels) if l == label]\n",
    "            if indices: final_prototypes[label] = np.sum(np.array(prototypes)[indices], axis=0)\n",
    "        self.class_prototypes = final_prototypes\n",
    "\n",
    "def post_process_predictions(predictions, window_size=5, merge_gap=30, fs=1.0):\n",
    "    smoothed = np.copy(predictions)\n",
    "    for i in range(len(predictions)):\n",
    "        start = max(0, i - window_size // 2); end = min(len(predictions), i + window_size // 2 + 1)\n",
    "        if np.mean(predictions[start:end]) < 0.5: smoothed[i] = 0\n",
    "    if merge_gap <= 0: return smoothed\n",
    "    merged = np.copy(smoothed)\n",
    "    change_indices = np.where(np.diff(smoothed) != 0)[0] + 1\n",
    "    seizure_blocks = []\n",
    "    if smoothed[0] == 1: seizure_blocks.append([0])\n",
    "    for idx in change_indices:\n",
    "        if smoothed[idx] == 1: seizure_blocks.append([idx])\n",
    "        else:\n",
    "            if len(seizure_blocks) > 0 and len(seizure_blocks[-1]) == 1:\n",
    "                seizure_blocks[-1].append(idx)\n",
    "    if len(seizure_blocks) > 0 and len(seizure_blocks[-1]) == 1: seizure_blocks[-1].append(len(smoothed))\n",
    "    for i in range(len(seizure_blocks) - 1):\n",
    "        end_first = seizure_blocks[i][1]; start_second = seizure_blocks[i+1][0]\n",
    "        gap_duration = (start_second - end_first) / fs\n",
    "        if gap_duration < merge_gap: merged[end_first:start_second] = 1\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb37771",
   "metadata": {},
   "source": [
    "Checagem da quantidade de arquivos com crise dentro dos 24 pacientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "900fccb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serviço do Google Drive conectado com sucesso.\n",
      "Caminho do dataset encontrado no Drive!\n",
      "Arquivo salvo em: ./temp_chb01_summary.txt\n",
      "chb01: 7 arquivos com crise | 7 crises\n",
      "Arquivo salvo em: ./temp_chb02_summary.txt\n",
      "chb02: 3 arquivos com crise | 3 crises\n",
      "Arquivo salvo em: ./temp_chb03_summary.txt\n",
      "chb03: 7 arquivos com crise | 7 crises\n",
      "Arquivo salvo em: ./temp_chb04_summary.txt\n",
      "chb04: 3 arquivos com crise | 4 crises\n",
      "Arquivo salvo em: ./temp_chb05_summary.txt\n",
      "chb05: 5 arquivos com crise | 5 crises\n",
      "Arquivo salvo em: ./temp_chb06_summary.txt\n",
      "chb06: 7 arquivos com crise | 10 crises\n",
      "Arquivo salvo em: ./temp_chb07_summary.txt\n",
      "chb07: 3 arquivos com crise | 3 crises\n",
      "Arquivo salvo em: ./temp_chb08_summary.txt\n",
      "chb08: 5 arquivos com crise | 5 crises\n",
      "Arquivo salvo em: ./temp_chb09_summary.txt\n",
      "chb09: 3 arquivos com crise | 4 crises\n",
      "Arquivo salvo em: ./temp_chb10_summary.txt\n",
      "chb10: 7 arquivos com crise | 7 crises\n",
      "Arquivo salvo em: ./temp_chb11_summary.txt\n",
      "chb11: 3 arquivos com crise | 3 crises\n",
      "Arquivo salvo em: ./temp_chb12_summary.txt\n",
      "chb12: 13 arquivos com crise | 40 crises\n",
      "Arquivo salvo em: ./temp_chb13_summary.txt\n",
      "chb13: 8 arquivos com crise | 12 crises\n",
      "Arquivo salvo em: ./temp_chb14_summary.txt\n",
      "chb14: 7 arquivos com crise | 8 crises\n",
      "Arquivo salvo em: ./temp_chb15_summary.txt\n",
      "chb15: 14 arquivos com crise | 20 crises\n",
      "Arquivo salvo em: ./temp_chb16_summary.txt\n",
      "chb16: 6 arquivos com crise | 10 crises\n",
      "Arquivo salvo em: ./temp_chb17_summary.txt\n",
      "chb17: 3 arquivos com crise | 3 crises\n",
      "Arquivo salvo em: ./temp_chb18_summary.txt\n",
      "chb18: 6 arquivos com crise | 6 crises\n",
      "Arquivo salvo em: ./temp_chb19_summary.txt\n",
      "chb19: 3 arquivos com crise | 3 crises\n",
      "Arquivo salvo em: ./temp_chb20_summary.txt\n",
      "chb20: 6 arquivos com crise | 8 crises\n",
      "Arquivo salvo em: ./temp_chb21_summary.txt\n",
      "chb21: 4 arquivos com crise | 4 crises\n",
      "Arquivo salvo em: ./temp_chb22_summary.txt\n",
      "chb22: 3 arquivos com crise | 3 crises\n",
      "Arquivo salvo em: ./temp_chb23_summary.txt\n",
      "chb23: 3 arquivos com crise | 7 crises\n",
      "Arquivo salvo em: ./temp_chb24_summary.txt\n",
      "chb24: 12 arquivos com crise | 16 crises\n",
      "\n",
      "=== RESUMO FINAL ===\n",
      "chb01: 7 arquivos com crise | 7 crises\n",
      "chb02: 3 arquivos com crise | 3 crises\n",
      "chb03: 7 arquivos com crise | 7 crises\n",
      "chb04: 3 arquivos com crise | 4 crises\n",
      "chb05: 5 arquivos com crise | 5 crises\n",
      "chb06: 7 arquivos com crise | 10 crises\n",
      "chb07: 3 arquivos com crise | 3 crises\n",
      "chb08: 5 arquivos com crise | 5 crises\n",
      "chb09: 3 arquivos com crise | 4 crises\n",
      "chb10: 7 arquivos com crise | 7 crises\n",
      "chb11: 3 arquivos com crise | 3 crises\n",
      "chb12: 13 arquivos com crise | 40 crises\n",
      "chb13: 8 arquivos com crise | 12 crises\n",
      "chb14: 7 arquivos com crise | 8 crises\n",
      "chb15: 14 arquivos com crise | 20 crises\n",
      "chb16: 6 arquivos com crise | 10 crises\n",
      "chb17: 3 arquivos com crise | 3 crises\n",
      "chb18: 6 arquivos com crise | 6 crises\n",
      "chb19: 3 arquivos com crise | 3 crises\n",
      "chb20: 6 arquivos com crise | 8 crises\n",
      "chb21: 4 arquivos com crise | 4 crises\n",
      "chb22: 3 arquivos com crise | 3 crises\n",
      "chb23: 3 arquivos com crise | 7 crises\n",
      "chb24: 12 arquivos com crise | 16 crises\n",
      "\n",
      "TOTAL de arquivos com crise: 141\n",
      "TOTAL de crises (eventos):   198\n"
     ]
    }
   ],
   "source": [
    "DRIVE_PATH_COMPONENTS = ['TCC EPILEPSIA DATA', 'chb-mit-scalp-eeg-database-1.0.0']\n",
    "\n",
    "service = get_drive_service()\n",
    "main_folder_id = find_folder_id_by_path(service, DRIVE_PATH_COMPONENTS)\n",
    "\n",
    "pacientes = [f\"chb{str(i).zfill(2)}\" for i in range(1, 25)]\n",
    "total_arquivos_com_crise = 0\n",
    "total_eventos = 0\n",
    "resumo = {}\n",
    "\n",
    "for paciente in pacientes:\n",
    "    try:\n",
    "        patient_folder_id = find_folder_id(service, paciente, parent_id=main_folder_id)\n",
    "        if not patient_folder_id:\n",
    "            print(f\"{paciente}: pasta não encontrada no Drive.\")\n",
    "            continue\n",
    "\n",
    "        drive_files = get_files_from_drive_folder(service, patient_folder_id)\n",
    "        summary_filename = f\"{paciente}-summary.txt\"\n",
    "        if summary_filename not in drive_files:\n",
    "            print(f\"{paciente}: summary não encontrado.\")\n",
    "            continue\n",
    "\n",
    "        local_summary_path = f\"./temp_{paciente}_summary.txt\"\n",
    "        download_file_locally(service, drive_files[summary_filename], local_summary_path)\n",
    "        seizure_times = parse_summary_file(local_summary_path)\n",
    "        os.remove(local_summary_path)\n",
    "\n",
    "        arquivos_com_crise = sum(1 for _, pares in seizure_times.items() if len(pares) > 0)\n",
    "        eventos = sum(len(pares) for pares in seizure_times.values())\n",
    "\n",
    "        resumo[paciente] = (arquivos_com_crise, eventos)\n",
    "        total_arquivos_com_crise += arquivos_com_crise\n",
    "        total_eventos += eventos\n",
    "\n",
    "        print(f\"{paciente}: {arquivos_com_crise} arquivos com crise | {eventos} crises\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro em {paciente}: {e}\")\n",
    "\n",
    "print(\"\\n=== RESUMO FINAL ===\")\n",
    "for p in pacientes:\n",
    "    a, e = resumo.get(p, (0, 0))\n",
    "    print(f\"{p}: {a} arquivos com crise | {e} crises\")\n",
    "\n",
    "print(f\"\\nTOTAL de arquivos com crise: {total_arquivos_com_crise}\")\n",
    "print(f\"TOTAL de crises (eventos):   {total_eventos}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387dab23",
   "metadata": {},
   "source": [
    "HDC - PADRÃO EM 5 PACIENTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "644072e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serviço do Google Drive conectado com sucesso.\n",
      "Caminho do dataset encontrado no Drive!\n",
      "\n",
      "=============== chb01 ===============\n",
      "Arquivo salvo em: ./temp_chb01_summary.txt\n",
      "chb01: Processando 7 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb01_03.edf\n",
      "Arquivo salvo em: ./temp_chb01_03.edf\n",
      "  Processando: chb01_04.edf\n",
      "Arquivo salvo em: ./temp_chb01_04.edf\n",
      "  Processando: chb01_15.edf\n",
      "Arquivo salvo em: ./temp_chb01_15.edf\n",
      "  Processando: chb01_16.edf\n",
      "Arquivo salvo em: ./temp_chb01_16.edf\n",
      "  Processando: chb01_18.edf\n",
      "Arquivo salvo em: ./temp_chb01_18.edf\n",
      "\n",
      ">>> Lote 2:\n",
      "  Processando: chb01_21.edf\n",
      "Arquivo salvo em: ./temp_chb01_21.edf\n",
      "  Processando: chb01_26.edf\n",
      "Arquivo salvo em: ./temp_chb01_26.edf\n",
      "\n",
      "=============== chb02 ===============\n",
      "Arquivo salvo em: ./temp_chb02_summary.txt\n",
      "chb02: Processando 3 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb02_16+.edf\n",
      "Arquivo salvo em: ./temp_chb02_16+.edf\n",
      "  Processando: chb02_16.edf\n",
      "Arquivo salvo em: ./temp_chb02_16.edf\n",
      "  Processando: chb02_19.edf\n",
      "Arquivo salvo em: ./temp_chb02_19.edf\n",
      "\n",
      "=============== chb03 ===============\n",
      "Arquivo salvo em: ./temp_chb03_summary.txt\n",
      "chb03: Processando 7 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb03_01.edf\n",
      "Arquivo salvo em: ./temp_chb03_01.edf\n",
      "  Processando: chb03_02.edf\n",
      "Arquivo salvo em: ./temp_chb03_02.edf\n",
      "  Processando: chb03_03.edf\n",
      "Arquivo salvo em: ./temp_chb03_03.edf\n",
      "  Processando: chb03_04.edf\n",
      "Arquivo salvo em: ./temp_chb03_04.edf\n",
      "  Processando: chb03_34.edf\n",
      "Arquivo salvo em: ./temp_chb03_34.edf\n",
      "\n",
      ">>> Lote 2:\n",
      "  Processando: chb03_35.edf\n",
      "Arquivo salvo em: ./temp_chb03_35.edf\n",
      "  Processando: chb03_36.edf\n",
      "Arquivo salvo em: ./temp_chb03_36.edf\n",
      "\n",
      "=============== chb04 ===============\n",
      "Arquivo salvo em: ./temp_chb04_summary.txt\n",
      "chb04: Processando 3 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb04_05.edf\n",
      "Arquivo salvo em: ./temp_chb04_05.edf\n",
      "  Processando: chb04_08.edf\n",
      "Arquivo salvo em: ./temp_chb04_08.edf\n",
      "  Processando: chb04_28.edf\n",
      "Arquivo salvo em: ./temp_chb04_28.edf\n",
      "\n",
      "=============== chb05 ===============\n",
      "Arquivo salvo em: ./temp_chb05_summary.txt\n",
      "chb05: Processando 5 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb05_06.edf\n",
      "Arquivo salvo em: ./temp_chb05_06.edf\n",
      "  Processando: chb05_13.edf\n",
      "Arquivo salvo em: ./temp_chb05_13.edf\n",
      "  Processando: chb05_16.edf\n",
      "Arquivo salvo em: ./temp_chb05_16.edf\n",
      "  Processando: chb05_17.edf\n",
      "Arquivo salvo em: ./temp_chb05_17.edf\n",
      "  Processando: chb05_22.edf\n",
      "  [ERRO] Falha ao processar chb05_22.edf: EOF occurred in violation of protocol (_ssl.c:2393)\n",
      "\n",
      "========================================\n",
      "    FASE DE TREINAMENTO E AVALIAÇÃO     \n",
      "========================================\n",
      "\n",
      "=== RESULTADO FINAL (HDC Padrão) ===\n",
      "F1: 0.6667 | Precisão: 0.5000 | Sensibilidade: 1.0000\n",
      "Matriz de Confusão:\n",
      " [[  0 558]\n",
      " [  0 558]]\n",
      "\n",
      "Tempo total de execução: 195.98 min\n"
     ]
    }
   ],
   "source": [
    "PATIENTS = [f\"chb{str(i).zfill(2)}\" for i in range(1, 6)]\n",
    "DRIVE_PATH_COMPONENTS = ['TCC EPILEPSIA DATA', 'chb-mit-scalp-eeg-database-1.0.0']\n",
    "\n",
    "DIMENSIONS = 10000\n",
    "NUM_LEVELS = 100\n",
    "\n",
    "WINDOW_SECONDS = 2\n",
    "MAX_CHANNELS = 20\n",
    "BATCH_SIZE = 5\n",
    "SEED = 42\n",
    "\n",
    "SMOOTHING_WINDOW_SIZE = 5\n",
    "MERGE_SEIZURES_THRESHOLD = 30\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    service = get_drive_service()\n",
    "    root_id = find_folder_id_by_path(service, DRIVE_PATH_COMPONENTS)\n",
    "\n",
    "    all_features, all_labels = [], []\n",
    "    fs_signal = -1 \n",
    "\n",
    "    for patient in PATIENTS:\n",
    "        print(f\"\\n{'='*15} {patient} {'='*15}\")\n",
    "        patient_id = find_folder_id(service, patient, parent_id=root_id)\n",
    "        files_map = get_files_from_drive_folder(service, patient_id)\n",
    "\n",
    "        summary_path = f\"./temp_{patient}_summary.txt\"\n",
    "        download_file_locally(service, files_map[f\"{patient}-summary.txt\"], summary_path)\n",
    "        seiz = parse_summary_file(summary_path)\n",
    "        os.remove(summary_path)\n",
    "\n",
    "        edf_with_seiz = sorted([f for f, ivals in seiz.items() if len(ivals) > 0])\n",
    "        print(f\"{patient}: Processando {len(edf_with_seiz)} arquivos com crise...\")\n",
    "\n",
    "        for k in range(0, len(edf_with_seiz), BATCH_SIZE):\n",
    "            batch = edf_with_seiz[k:k+BATCH_SIZE]\n",
    "            print(f\"\\n>>> Lote {k//BATCH_SIZE + 1}:\")\n",
    "\n",
    "            for edf_name in batch:\n",
    "                local_path = f\"./temp_{edf_name}\"\n",
    "                print(f\"  Processando: {edf_name}\")\n",
    "                try:\n",
    "                    download_file_locally(service, files_map[edf_name], local_path)\n",
    "                    with pyedflib.EdfReader(local_path) as r:\n",
    "                        fs_signal = r.getSampleFrequency(0)\n",
    "                        n_ch = min(int(MAX_CHANNELS), r.signals_in_file)\n",
    "                        signals = np.array([r.readSignal(c) for c in range(n_ch)], dtype=np.float32)\n",
    "\n",
    "                        win_samples = int(fs_signal * WINDOW_SECONDS)\n",
    "                        step = win_samples // 2\n",
    "                        \n",
    "                        for j in range(0, signals.shape[1] - win_samples + 1, step):\n",
    "                            window = signals[:, j : j + win_samples]\n",
    "                            start_time_sec, end_time_sec = j / fs_signal, (j + win_samples) / fs_signal\n",
    "                            is_seiz = any(max(start_time_sec, s_start) < min(end_time_sec, s_end) for s_start, s_end in seiz.get(edf_name, []))\n",
    "                            feats = np.mean([extract_single_feature_vector(window[c, :], fs_signal) for c in range(n_ch)], axis=0)\n",
    "                            all_features.append(feats); all_labels.append(1 if is_seiz else 0)\n",
    "                except Exception as e:\n",
    "                    print(f\"  [ERRO] Falha ao processar {edf_name}: {e}\")\n",
    "                finally:\n",
    "                    if os.path.exists(local_path): os.remove(local_path)\n",
    "            gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\" + \"FASE DE TREINAMENTO E AVALIAÇÃO\".center(40) + \"\\n\" + \"=\"*40)\n",
    "    \n",
    "    X = np.array(all_features); y = np.array(all_labels)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    X_pos, X_neg = X_scaled[y == 1], X_scaled[y == 0]\n",
    "    X_neg_balanced = resample(X_neg, replace=False, n_samples=len(X_pos), random_state=SEED)\n",
    "    X_balanced = np.vstack([X_pos, X_neg_balanced])\n",
    "    y_balanced = np.hstack([np.ones(len(X_pos)), np.zeros(len(X_pos))])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.3, stratify=y_balanced, random_state=SEED)\n",
    "\n",
    "    NUM_FEATURES = X_train.shape[1]\n",
    "    \n",
    "    encoder = HDC(DIMENSIONS, NUM_FEATURES, NUM_LEVELS, seed=SEED)\n",
    "    X_train_hd = encoder.encode(X_train)\n",
    "    X_test_hd = encoder.encode(X_test)\n",
    "\n",
    "    model = HDC(DIMENSIONS, NUM_FEATURES, NUM_LEVELS, seed=SEED)\n",
    "    model.train_standard(X_train_hd, y_train)\n",
    "    y_pred_raw = model.predict(X_test_hd)\n",
    "\n",
    "    win_samples_final = int(fs_signal * WINDOW_SECONDS)\n",
    "    step_final = win_samples_final // 2\n",
    "    fs_windows = fs_signal / step_final\n",
    "    y_pred_pp = post_process_predictions(y_pred_raw, window_size=SMOOTHING_WINDOW_SIZE,\n",
    "                                       merge_gap=MERGE_SEIZURES_THRESHOLD, fs=fs_windows)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred_pp, labels=[0,1])\n",
    "    print(\"\\n=== RESULTADO FINAL (HDC Padrão) ===\")\n",
    "    print(f\"F1: {f1_score(y_test, y_pred_pp, zero_division=0):.4f} | Precisão: {precision_score(y_test, y_pred_pp, zero_division=0):.4f} | Sensibilidade: {recall_score(y_test, y_pred_pp, zero_division=0):.4f}\")\n",
    "    print(\"Matriz de Confusão:\\n\", cm)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\nERRO GERAL:\", e)\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTempo total de execução: {(time.time()-start_time)/60:.2f} min\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ae6319",
   "metadata": {},
   "source": [
    "HDC - MULTI-PASS EM 5 PACIENTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "43691068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serviço do Google Drive conectado com sucesso.\n",
      "Caminho do dataset encontrado no Drive!\n",
      "\n",
      "=============== chb01 ===============\n",
      "Arquivo salvo em: ./temp_chb01_summary.txt\n",
      "chb01: Processando 7 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb01_03.edf\n",
      "Arquivo salvo em: ./temp_chb01_03.edf\n",
      "  Processando: chb01_04.edf\n",
      "Arquivo salvo em: ./temp_chb01_04.edf\n",
      "  Processando: chb01_15.edf\n",
      "Arquivo salvo em: ./temp_chb01_15.edf\n",
      "  Processando: chb01_16.edf\n",
      "Arquivo salvo em: ./temp_chb01_16.edf\n",
      "  Processando: chb01_18.edf\n",
      "Arquivo salvo em: ./temp_chb01_18.edf\n",
      "\n",
      ">>> Lote 2:\n",
      "  Processando: chb01_21.edf\n",
      "Arquivo salvo em: ./temp_chb01_21.edf\n",
      "  Processando: chb01_26.edf\n",
      "Arquivo salvo em: ./temp_chb01_26.edf\n",
      "\n",
      "=============== chb02 ===============\n",
      "Arquivo salvo em: ./temp_chb02_summary.txt\n",
      "chb02: Processando 3 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb02_16+.edf\n",
      "Arquivo salvo em: ./temp_chb02_16+.edf\n",
      "  Processando: chb02_16.edf\n",
      "Arquivo salvo em: ./temp_chb02_16.edf\n",
      "  Processando: chb02_19.edf\n",
      "Arquivo salvo em: ./temp_chb02_19.edf\n",
      "\n",
      "=============== chb03 ===============\n",
      "Arquivo salvo em: ./temp_chb03_summary.txt\n",
      "chb03: Processando 7 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb03_01.edf\n",
      "Arquivo salvo em: ./temp_chb03_01.edf\n",
      "  Processando: chb03_02.edf\n",
      "Arquivo salvo em: ./temp_chb03_02.edf\n",
      "  Processando: chb03_03.edf\n",
      "Arquivo salvo em: ./temp_chb03_03.edf\n",
      "  Processando: chb03_04.edf\n",
      "Arquivo salvo em: ./temp_chb03_04.edf\n",
      "  Processando: chb03_34.edf\n",
      "Arquivo salvo em: ./temp_chb03_34.edf\n",
      "\n",
      ">>> Lote 2:\n",
      "  Processando: chb03_35.edf\n",
      "Arquivo salvo em: ./temp_chb03_35.edf\n",
      "  Processando: chb03_36.edf\n",
      "Arquivo salvo em: ./temp_chb03_36.edf\n",
      "\n",
      "=============== chb04 ===============\n",
      "Arquivo salvo em: ./temp_chb04_summary.txt\n",
      "chb04: Processando 3 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb04_05.edf\n",
      "Arquivo salvo em: ./temp_chb04_05.edf\n",
      "  Processando: chb04_08.edf\n",
      "Arquivo salvo em: ./temp_chb04_08.edf\n",
      "  Processando: chb04_28.edf\n",
      "Arquivo salvo em: ./temp_chb04_28.edf\n",
      "\n",
      "=============== chb05 ===============\n",
      "Arquivo salvo em: ./temp_chb05_summary.txt\n",
      "chb05: Processando 5 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb05_06.edf\n",
      "Arquivo salvo em: ./temp_chb05_06.edf\n",
      "  Processando: chb05_13.edf\n",
      "Arquivo salvo em: ./temp_chb05_13.edf\n",
      "  Processando: chb05_16.edf\n",
      "Arquivo salvo em: ./temp_chb05_16.edf\n",
      "  Processando: chb05_17.edf\n",
      "Arquivo salvo em: ./temp_chb05_17.edf\n",
      "  Processando: chb05_22.edf\n",
      "Arquivo salvo em: ./temp_chb05_22.edf\n",
      "\n",
      "========================================\n",
      "    FASE DE TREINAMENTO E AVALIAÇÃO     \n",
      "========================================\n",
      "\n",
      "=== RESULTADO FINAL (HDC Multi-Pass) ===\n",
      "F1: 0.6476 | Precisão: 0.5123 | Sensibilidade: 0.8803\n",
      "Matriz de Confusão:\n",
      " [[ 97 497]\n",
      " [ 71 522]]\n",
      "\n",
      "Tempo total de execução: 213.38 min\n"
     ]
    }
   ],
   "source": [
    "PATIENTS = [f\"chb{str(i).zfill(2)}\" for i in range(1, 6)] \n",
    "DRIVE_PATH_COMPONENTS = ['TCC EPILEPSIA DATA', 'chb-mit-scalp-eeg-database-1.0.0']\n",
    "\n",
    "DIMENSIONS = 10000; NUM_LEVELS = 100; EPOCHS_MULTIPASS = 12\n",
    "LEARNING_RATE_MULTIPASS = 0.1\n",
    "\n",
    "WINDOW_SECONDS = 2; MAX_CHANNELS = 20; BATCH_SIZE = 5; SEED = 42\n",
    "\n",
    "SMOOTHING_WINDOW_SIZE = 5; MERGE_SEIZURES_THRESHOLD = 30\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    service = get_drive_service()\n",
    "    root_id = find_folder_id_by_path(service, DRIVE_PATH_COMPONENTS)\n",
    "\n",
    "    all_features, all_labels = [], []\n",
    "    fs_signal = -1\n",
    "\n",
    "    for patient in PATIENTS:\n",
    "        print(f\"\\n{'='*15} {patient} {'='*15}\")\n",
    "        patient_id = find_folder_id(service, patient, parent_id=root_id)\n",
    "        files_map = get_files_from_drive_folder(service, patient_id)\n",
    "\n",
    "        summary_path = f\"./temp_{patient}_summary.txt\"\n",
    "        download_file_locally(service, files_map[f\"{patient}-summary.txt\"], summary_path)\n",
    "        seiz = parse_summary_file(summary_path)\n",
    "        os.remove(summary_path)\n",
    "\n",
    "        edf_with_seiz = sorted([f for f, ivals in seiz.items() if len(ivals) > 0])\n",
    "        print(f\"{patient}: Processando {len(edf_with_seiz)} arquivos com crise...\")\n",
    "\n",
    "        for k in range(0, len(edf_with_seiz), BATCH_SIZE):\n",
    "            batch = edf_with_seiz[k:k+BATCH_SIZE]\n",
    "            print(f\"\\n>>> Lote {k//BATCH_SIZE + 1}:\")\n",
    "\n",
    "            for edf_name in batch:\n",
    "                local_path = f\"./temp_{edf_name}\"\n",
    "                print(f\"  Processando: {edf_name}\")\n",
    "                try:\n",
    "                    download_file_locally(service, files_map[edf_name], local_path)\n",
    "                    with pyedflib.EdfReader(local_path) as r:\n",
    "                        fs_signal = r.getSampleFrequency(0)\n",
    "                        n_ch = min(int(MAX_CHANNELS), r.signals_in_file)\n",
    "                        signals = np.array([r.readSignal(c) for c in range(n_ch)], dtype=np.float32)\n",
    "                        win_samples = int(fs_signal * WINDOW_SECONDS); step = win_samples // 2\n",
    "                        for j in range(0, signals.shape[1] - win_samples + 1, step):\n",
    "                            window = signals[:, j : j + win_samples]\n",
    "                            start_time_sec, end_time_sec = j / fs_signal, (j + win_samples) / fs_signal\n",
    "                            is_seiz = any(max(start_time_sec, s_start) < min(end_time_sec, s_end) for s_start, s_end in seiz.get(edf_name, []))\n",
    "                            feats = np.mean([extract_single_feature_vector(window[c, :], fs_signal) for c in range(n_ch)], axis=0)\n",
    "                            all_features.append(feats); all_labels.append(1 if is_seiz else 0)\n",
    "                except Exception as e: print(f\"  [ERRO] Falha ao processar {edf_name}: {e}\")\n",
    "                finally:\n",
    "                    if os.path.exists(local_path): os.remove(local_path)\n",
    "            gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\" + \"FASE DE TREINAMENTO E AVALIAÇÃO\".center(40) + \"\\n\" + \"=\"*40)\n",
    "    \n",
    "    X = np.array(all_features); y = np.array(all_labels)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    X_pos, X_neg = X_scaled[y == 1], X_scaled[y == 0]\n",
    "    if len(X_pos) == 0: raise ValueError(\"Nenhuma amostra de crise foi encontrada.\")\n",
    "        \n",
    "    X_neg_balanced = resample(X_neg, replace=False, n_samples=len(X_pos), random_state=SEED)\n",
    "    X_balanced = np.vstack([X_pos, X_neg_balanced])\n",
    "    \n",
    "    y_balanced = np.hstack([np.ones(len(X_pos)), np.zeros(len(X_pos))]).astype(int)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.3, stratify=y_balanced, random_state=SEED)\n",
    "\n",
    "    NUM_FEATURES = X_train.shape[1]\n",
    "    \n",
    "    encoder = HDC(DIMENSIONS, NUM_FEATURES, NUM_LEVELS, seed=SEED)\n",
    "    X_train_hd = encoder.encode(X_train)\n",
    "    X_test_hd = encoder.encode(X_test)\n",
    "\n",
    "    model = HDC(DIMENSIONS, NUM_FEATURES, NUM_LEVELS, seed=SEED)\n",
    "    model.train_multipass(X_train_hd, y_train, epochs=EPOCHS_MULTIPASS, lr=LEARNING_RATE_MULTIPASS, subtract_wrong=True)\n",
    "    y_pred_raw = model.predict(X_test_hd)\n",
    "\n",
    "    win_samples_final = int(fs_signal * WINDOW_SECONDS)\n",
    "    step_final = win_samples_final // 2\n",
    "    fs_windows = fs_signal / step_final \n",
    "    y_pred_pp = post_process_predictions(y_pred_raw, window_size=SMOOTHING_WINDOW_SIZE,\n",
    "                                       merge_gap=MERGE_SEIZURES_THRESHOLD, fs=fs_windows)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred_pp, labels=[0,1])\n",
    "    print(\"\\n=== RESULTADO FINAL (HDC Multi-Pass) ===\")\n",
    "    print(f\"F1: {f1_score(y_test, y_pred_pp, zero_division=0):.4f} | Precisão: {precision_score(y_test, y_pred_pp, zero_division=0):.4f} | Sensibilidade: {recall_score(y_test, y_pred_pp, zero_division=0):.4f}\")\n",
    "    print(\"Matriz de Confusão:\\n\", cm)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTempo total de execução: {(end_time-start_time)/60:.2f} min\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eccc8e",
   "metadata": {},
   "source": [
    "HDC - MULTI-CENTROID EM 5 PACIENTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db06418d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serviço do Google Drive conectado com sucesso.\n",
      "Caminho do dataset encontrado no Drive!\n",
      "\n",
      "=============== chb01 ===============\n",
      "Arquivo salvo em: ./temp_chb01_summary.txt\n",
      "chb01: Processando 7 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb01_03.edf\n",
      "Arquivo salvo em: ./temp_chb01_03.edf\n",
      "  Processando: chb01_04.edf\n",
      "Arquivo salvo em: ./temp_chb01_04.edf\n",
      "  Processando: chb01_15.edf\n",
      "Arquivo salvo em: ./temp_chb01_15.edf\n",
      "  Processando: chb01_16.edf\n",
      "Arquivo salvo em: ./temp_chb01_16.edf\n",
      "  Processando: chb01_18.edf\n",
      "Arquivo salvo em: ./temp_chb01_18.edf\n",
      "\n",
      ">>> Lote 2:\n",
      "  Processando: chb01_21.edf\n",
      "Arquivo salvo em: ./temp_chb01_21.edf\n",
      "  Processando: chb01_26.edf\n",
      "Arquivo salvo em: ./temp_chb01_26.edf\n",
      "\n",
      "=============== chb02 ===============\n",
      "Arquivo salvo em: ./temp_chb02_summary.txt\n",
      "chb02: Processando 3 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb02_16+.edf\n",
      "Arquivo salvo em: ./temp_chb02_16+.edf\n",
      "  Processando: chb02_16.edf\n",
      "Arquivo salvo em: ./temp_chb02_16.edf\n",
      "  Processando: chb02_19.edf\n",
      "Arquivo salvo em: ./temp_chb02_19.edf\n",
      "\n",
      "=============== chb03 ===============\n",
      "Arquivo salvo em: ./temp_chb03_summary.txt\n",
      "chb03: Processando 7 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb03_01.edf\n",
      "Arquivo salvo em: ./temp_chb03_01.edf\n",
      "  Processando: chb03_02.edf\n",
      "Arquivo salvo em: ./temp_chb03_02.edf\n",
      "  Processando: chb03_03.edf\n",
      "Arquivo salvo em: ./temp_chb03_03.edf\n",
      "  Processando: chb03_04.edf\n",
      "Arquivo salvo em: ./temp_chb03_04.edf\n",
      "  Processando: chb03_34.edf\n",
      "Arquivo salvo em: ./temp_chb03_34.edf\n",
      "\n",
      ">>> Lote 2:\n",
      "  Processando: chb03_35.edf\n",
      "Arquivo salvo em: ./temp_chb03_35.edf\n",
      "  Processando: chb03_36.edf\n",
      "Arquivo salvo em: ./temp_chb03_36.edf\n",
      "\n",
      "=============== chb04 ===============\n",
      "Arquivo salvo em: ./temp_chb04_summary.txt\n",
      "chb04: Processando 3 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb04_05.edf\n",
      "Arquivo salvo em: ./temp_chb04_05.edf\n",
      "  Processando: chb04_08.edf\n",
      "Arquivo salvo em: ./temp_chb04_08.edf\n",
      "  Processando: chb04_28.edf\n",
      "Arquivo salvo em: ./temp_chb04_28.edf\n",
      "\n",
      "=============== chb05 ===============\n",
      "Arquivo salvo em: ./temp_chb05_summary.txt\n",
      "chb05: Processando 5 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb05_06.edf\n",
      "Arquivo salvo em: ./temp_chb05_06.edf\n",
      "  Processando: chb05_13.edf\n",
      "Arquivo salvo em: ./temp_chb05_13.edf\n",
      "  Processando: chb05_16.edf\n",
      "Arquivo salvo em: ./temp_chb05_16.edf\n",
      "  Processando: chb05_17.edf\n",
      "Arquivo salvo em: ./temp_chb05_17.edf\n",
      "  Processando: chb05_22.edf\n",
      "Arquivo salvo em: ./temp_chb05_22.edf\n",
      "\n",
      "========================================\n",
      "    FASE DE TREINAMENTO E AVALIAÇÃO     \n",
      "========================================\n",
      "\n",
      "=== RESULTADO FINAL (HDC Multi-Centroid) ===\n",
      "F1: 0.6693 | Precisão: 0.5038 | Sensibilidade: 0.9966\n",
      "Matriz de Confusão:\n",
      " [[ 12 582]\n",
      " [  2 591]]\n",
      "\n",
      "Tempo total de execução: 206.78 min\n"
     ]
    }
   ],
   "source": [
    "PATIENTS = [f\"chb{str(i).zfill(2)}\" for i in range(1, 6)]\n",
    "DRIVE_PATH_COMPONENTS = ['TCC EPILEPSIA DATA', 'chb-mit-scalp-eeg-database-1.0.0']\n",
    "\n",
    "DIMENSIONS = 10000; NUM_LEVELS = 100; THRESHOLD_MULTICENTROID = 0.25\n",
    "\n",
    "WINDOW_SECONDS = 2; MAX_CHANNELS = 20; BATCH_SIZE = 5; SEED = 42\n",
    "\n",
    "SMOOTHING_WINDOW_SIZE = 5; MERGE_SEIZURES_THRESHOLD = 30\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    service = get_drive_service()\n",
    "    root_id = find_folder_id_by_path(service, DRIVE_PATH_COMPONENTS)\n",
    "\n",
    "    all_features, all_labels = [], []\n",
    "    fs_signal = -1\n",
    "\n",
    "    for patient in PATIENTS:\n",
    "        print(f\"\\n{'='*15} {patient} {'='*15}\")\n",
    "        patient_id = find_folder_id(service, patient, parent_id=root_id)\n",
    "        files_map = get_files_from_drive_folder(service, patient_id)\n",
    "\n",
    "        summary_path = f\"./temp_{patient}_summary.txt\"\n",
    "        download_file_locally(service, files_map[f\"{patient}-summary.txt\"], summary_path)\n",
    "        seiz = parse_summary_file(summary_path)\n",
    "        os.remove(summary_path)\n",
    "\n",
    "        edf_with_seiz = sorted([f for f, ivals in seiz.items() if len(ivals) > 0])\n",
    "        print(f\"{patient}: Processando {len(edf_with_seiz)} arquivos com crise...\")\n",
    "\n",
    "        for k in range(0, len(edf_with_seiz), BATCH_SIZE):\n",
    "            batch = edf_with_seiz[k:k+BATCH_SIZE]\n",
    "            print(f\"\\n>>> Lote {k//BATCH_SIZE + 1}:\")\n",
    "\n",
    "            for edf_name in batch:\n",
    "                local_path = f\"./temp_{edf_name}\"\n",
    "                print(f\"  Processando: {edf_name}\")\n",
    "                try:\n",
    "                    download_file_locally(service, files_map[edf_name], local_path)\n",
    "                    with pyedflib.EdfReader(local_path) as r:\n",
    "                        fs_signal = r.getSampleFrequency(0)\n",
    "                        n_ch = min(int(MAX_CHANNELS), r.signals_in_file)\n",
    "                        signals = np.array([r.readSignal(c) for c in range(n_ch)], dtype=np.float32)\n",
    "\n",
    "                        win_samples = int(fs_signal * WINDOW_SECONDS)\n",
    "                        step = win_samples // 2\n",
    "                        \n",
    "                        for j in range(0, signals.shape[1] - win_samples + 1, step):\n",
    "                            window = signals[:, j : j + win_samples]\n",
    "                            start_time_sec, end_time_sec = j / fs_signal, (j + win_samples) / fs_signal\n",
    "                            is_seiz = any(max(start_time_sec, s_start) < min(end_time_sec, s_end) for s_start, s_end in seiz.get(edf_name, []))\n",
    "                            feats = np.mean([extract_single_feature_vector(window[c, :], fs_signal) for c in range(n_ch)], axis=0)\n",
    "                            all_features.append(feats)\n",
    "                            all_labels.append(1 if is_seiz else 0)\n",
    "                except Exception as e: print(f\"  [ERRO] Falha ao processar {edf_name}: {e}\")\n",
    "                finally:\n",
    "                    if os.path.exists(local_path): os.remove(local_path)\n",
    "            gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\" + \"FASE DE TREINAMENTO E AVALIAÇÃO\".center(40) + \"\\n\" + \"=\"*40)\n",
    "    \n",
    "    X = np.array(all_features); y = np.array(all_labels)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    X_pos, X_neg = X_scaled[y == 1], X_scaled[y == 0]\n",
    "    if len(X_pos) == 0: raise ValueError(\"Nenhuma amostra de crise foi encontrada.\")\n",
    "        \n",
    "    X_neg_balanced = resample(X_neg, replace=False, n_samples=len(X_pos), random_state=SEED)\n",
    "    X_balanced = np.vstack([X_pos, X_neg_balanced])\n",
    "    \n",
    "    y_balanced = np.hstack([np.ones(len(X_pos)), np.zeros(len(X_pos))]).astype(int)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.3, stratify=y_balanced, random_state=SEED)\n",
    "\n",
    "    NUM_FEATURES = X_train.shape[1]\n",
    "    \n",
    "    encoder = HDC(DIMENSIONS, NUM_FEATURES, NUM_LEVELS, seed=SEED)\n",
    "    X_train_hd = encoder.encode(X_train)\n",
    "    X_test_hd = encoder.encode(X_test)\n",
    "\n",
    "    model = HDC(DIMENSIONS, NUM_FEATURES, NUM_LEVELS, seed=SEED)\n",
    "    model.train_multicentroid(X_train_hd, y_train, threshold=THRESHOLD_MULTICENTROID)\n",
    "    y_pred_raw = model.predict(X_test_hd)\n",
    "\n",
    "    win_samples_final = int(fs_signal * WINDOW_SECONDS)\n",
    "    step_final = win_samples_final // 2\n",
    "    fs_windows = fs_signal / step_final \n",
    "    y_pred_pp = post_process_predictions(y_pred_raw, window_size=SMOOTHING_WINDOW_SIZE,\n",
    "                                       merge_gap=MERGE_SEIZURES_THRESHOLD, fs=fs_windows)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred_pp, labels=[0,1])\n",
    "    print(\"\\n=== RESULTADO FINAL (HDC Multi-Centroid) ===\")\n",
    "    print(f\"F1: {f1_score(y_test, y_pred_pp, zero_division=0):.4f} | Precisão: {precision_score(y_test, y_pred_pp, zero_division=0):.4f} | Sensibilidade: {recall_score(y_test, y_pred_pp, zero_division=0):.4f}\")\n",
    "    print(\"Matriz de Confusão:\\n\", cm)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTempo total de execução: {(time.time()-start_time)/60:.2f} min\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e981b09",
   "metadata": {},
   "source": [
    "HDC - MP + MC EM 5 PACIENTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f80d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serviço do Google Drive conectado com sucesso.\n",
      "Caminho do dataset encontrado no Drive!\n",
      "\n",
      "=============== chb01 ===============\n",
      "Arquivo salvo em: ./temp_chb01_summary.txt\n",
      "chb01: Processando 7 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb01_03.edf\n",
      "Arquivo salvo em: ./temp_chb01_03.edf\n",
      "  Processando: chb01_04.edf\n",
      "Arquivo salvo em: ./temp_chb01_04.edf\n",
      "  Processando: chb01_15.edf\n",
      "Arquivo salvo em: ./temp_chb01_15.edf\n",
      "  Processando: chb01_16.edf\n",
      "Arquivo salvo em: ./temp_chb01_16.edf\n",
      "  Processando: chb01_18.edf\n",
      "Arquivo salvo em: ./temp_chb01_18.edf\n",
      "\n",
      ">>> Lote 2:\n",
      "  Processando: chb01_21.edf\n",
      "Arquivo salvo em: ./temp_chb01_21.edf\n",
      "  Processando: chb01_26.edf\n",
      "Arquivo salvo em: ./temp_chb01_26.edf\n",
      "\n",
      "=============== chb02 ===============\n",
      "Arquivo salvo em: ./temp_chb02_summary.txt\n",
      "chb02: Processando 3 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb02_16+.edf\n",
      "Arquivo salvo em: ./temp_chb02_16+.edf\n",
      "  Processando: chb02_16.edf\n",
      "Arquivo salvo em: ./temp_chb02_16.edf\n",
      "  Processando: chb02_19.edf\n",
      "Arquivo salvo em: ./temp_chb02_19.edf\n",
      "\n",
      "=============== chb03 ===============\n",
      "Arquivo salvo em: ./temp_chb03_summary.txt\n",
      "chb03: Processando 7 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb03_01.edf\n",
      "Arquivo salvo em: ./temp_chb03_01.edf\n",
      "  Processando: chb03_02.edf\n",
      "Arquivo salvo em: ./temp_chb03_02.edf\n",
      "  Processando: chb03_03.edf\n",
      "Arquivo salvo em: ./temp_chb03_03.edf\n",
      "  Processando: chb03_04.edf\n",
      "Arquivo salvo em: ./temp_chb03_04.edf\n",
      "  Processando: chb03_34.edf\n",
      "Arquivo salvo em: ./temp_chb03_34.edf\n",
      "\n",
      ">>> Lote 2:\n",
      "  Processando: chb03_35.edf\n",
      "Arquivo salvo em: ./temp_chb03_35.edf\n",
      "  Processando: chb03_36.edf\n",
      "Arquivo salvo em: ./temp_chb03_36.edf\n",
      "\n",
      "=============== chb04 ===============\n",
      "Arquivo salvo em: ./temp_chb04_summary.txt\n",
      "chb04: Processando 3 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb04_05.edf\n",
      "Arquivo salvo em: ./temp_chb04_05.edf\n",
      "  Processando: chb04_08.edf\n",
      "Arquivo salvo em: ./temp_chb04_08.edf\n",
      "  Processando: chb04_28.edf\n",
      "Arquivo salvo em: ./temp_chb04_28.edf\n",
      "\n",
      "=============== chb05 ===============\n",
      "Arquivo salvo em: ./temp_chb05_summary.txt\n",
      "chb05: Processando 5 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb05_06.edf\n",
      "Arquivo salvo em: ./temp_chb05_06.edf\n",
      "  Processando: chb05_13.edf\n",
      "Arquivo salvo em: ./temp_chb05_13.edf\n",
      "  Processando: chb05_16.edf\n",
      "Arquivo salvo em: ./temp_chb05_16.edf\n",
      "  Processando: chb05_17.edf\n",
      "Arquivo salvo em: ./temp_chb05_17.edf\n",
      "  Processando: chb05_22.edf\n",
      "Arquivo salvo em: ./temp_chb05_22.edf\n",
      "\n",
      "========================================\n",
      "    FASE DE TREINAMENTO E AVALIAÇÃO     \n",
      "========================================\n",
      "\n",
      "=== RESULTADO FINAL (HDC MC + MP) ===\n",
      "F1: 0.6553 | Precisão: 0.5118 | Sensibilidade: 0.9106\n",
      "Matriz de Confusão:\n",
      " [[ 79 515]\n",
      " [ 53 540]]\n",
      "\n",
      "Tempo total de execução: 204.62 min\n"
     ]
    }
   ],
   "source": [
    "PATIENTS = [f\"chb{str(i).zfill(2)}\" for i in range(1, 6)]\n",
    "DRIVE_PATH_COMPONENTS = ['TCC EPILEPSIA DATA', 'chb-mit-scalp-eeg-database-1.0.0']\n",
    "\n",
    "DIMENSIONS = 10000; NUM_LEVELS = 100; EPOCHS_MULTIPASS = 12\n",
    "LEARNING_RATE_MULTIPASS = 0.1; THRESHOLD_MULTICENTROID = 0.25\n",
    "\n",
    "WINDOW_SECONDS = 2; MAX_CHANNELS = 20; BATCH_SIZE = 5; SEED = 42\n",
    "\n",
    "SMOOTHING_WINDOW_SIZE = 5; MERGE_SEIZURES_THRESHOLD = 30\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    service = get_drive_service()\n",
    "    root_id = find_folder_id_by_path(service, DRIVE_PATH_COMPONENTS)\n",
    "\n",
    "    all_features, all_labels = [], []\n",
    "    fs_signal = -1\n",
    "\n",
    "    for patient in PATIENTS:\n",
    "        print(f\"\\n{'='*15} {patient} {'='*15}\")\n",
    "        patient_id = find_folder_id(service, patient, parent_id=root_id)\n",
    "        files_map = get_files_from_drive_folder(service, patient_id)\n",
    "\n",
    "        summary_path = f\"./temp_{patient}_summary.txt\"\n",
    "        download_file_locally(service, files_map[f\"{patient}-summary.txt\"], summary_path)\n",
    "        seiz = parse_summary_file(summary_path)\n",
    "        os.remove(summary_path)\n",
    "\n",
    "        edf_with_seiz = sorted([f for f, ivals in seiz.items() if len(ivals) > 0])\n",
    "        print(f\"{patient}: Processando {len(edf_with_seiz)} arquivos com crise...\")\n",
    "\n",
    "        for k in range(0, len(edf_with_seiz), BATCH_SIZE):\n",
    "            batch = edf_with_seiz[k:k+BATCH_SIZE]\n",
    "            print(f\"\\n>>> Lote {k//BATCH_SIZE + 1}:\")\n",
    "\n",
    "            for edf_name in batch:\n",
    "                local_path = f\"./temp_{edf_name}\"\n",
    "                print(f\"  Processando: {edf_name}\")\n",
    "                try:\n",
    "                    download_file_locally(service, files_map[edf_name], local_path)\n",
    "                    with pyedflib.EdfReader(local_path) as r:\n",
    "                        fs_signal = r.getSampleFrequency(0)\n",
    "                        n_ch = min(int(MAX_CHANNELS), r.signals_in_file)\n",
    "                        signals = np.array([r.readSignal(c) for c in range(n_ch)], dtype=np.float32)\n",
    "\n",
    "                        win_samples = int(fs_signal * WINDOW_SECONDS)\n",
    "                        step = win_samples // 2\n",
    "                        \n",
    "                        for j in range(0, signals.shape[1] - win_samples + 1, step):\n",
    "                            window = signals[:, j : j + win_samples]\n",
    "                            start_time_sec, end_time_sec = j / fs_signal, (j + win_samples) / fs_signal\n",
    "                            is_seiz = any(max(start_time_sec, s_start) < min(end_time_sec, s_end) for s_start, s_end in seiz.get(edf_name, []))\n",
    "                            feats = np.mean([extract_single_feature_vector(window[c, :], fs_signal) for c in range(n_ch)], axis=0)\n",
    "                            all_features.append(feats); all_labels.append(1 if is_seiz else 0)\n",
    "                except Exception as e: print(f\"  [ERRO] Falha ao processar {edf_name}: {e}\")\n",
    "                finally:\n",
    "                    if os.path.exists(local_path): os.remove(local_path)\n",
    "            gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\" + \"FASE DE TREINAMENTO E AVALIAÇÃO\".center(40) + \"\\n\" + \"=\"*40)\n",
    "    \n",
    "    X = np.array(all_features); y = np.array(all_labels)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    X_pos, X_neg = X_scaled[y == 1], X_scaled[y == 0]\n",
    "    if len(X_pos) == 0: raise ValueError(\"Nenhuma amostra de crise foi encontrada.\")\n",
    "        \n",
    "    X_neg_balanced = resample(X_neg, replace=False, n_samples=len(X_pos), random_state=SEED)\n",
    "    X_balanced = np.vstack([X_pos, X_neg_balanced])\n",
    "    \n",
    "    y_balanced = np.hstack([np.ones(len(X_pos)), np.zeros(len(X_pos))]).astype(int)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.3, stratify=y_balanced, random_state=SEED)\n",
    "\n",
    "    NUM_FEATURES = X_train.shape[1]\n",
    "    \n",
    "    encoder = HDC(DIMENSIONS, NUM_FEATURES, NUM_LEVELS, seed=SEED)\n",
    "    X_train_hd = encoder.encode(X_train)\n",
    "    X_test_hd = encoder.encode(X_test)\n",
    "\n",
    "    model = HDC(DIMENSIONS, NUM_FEATURES, NUM_LEVELS, seed=SEED)\n",
    "    model.train_multicentroid(X_train_hd, y_train, threshold=THRESHOLD_MULTICENTROID)\n",
    "    model.train_multipass(X_train_hd, y_train, epochs=EPOCHS_MULTIPASS, lr=LEARNING_RATE_MULTIPASS, subtract_wrong=True, initial_training=False)\n",
    "    y_pred_raw = model.predict(X_test_hd)\n",
    "\n",
    "    win_samples_final = int(fs_signal * WINDOW_SECONDS)\n",
    "    step_final = win_samples_final // 2\n",
    "    fs_windows = fs_signal / step_final \n",
    "    y_pred_pp = post_process_predictions(y_pred_raw, window_size=SMOOTHING_WINDOW_SIZE,\n",
    "                                       merge_gap=MERGE_SEIZURES_THRESHOLD, fs=fs_windows)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred_pp, labels=[0,1])\n",
    "    print(\"\\n=== RESULTADO FINAL (HDC MC + MP) ===\")\n",
    "    print(f\"F1: {f1_score(y_test, y_pred_pp, zero_division=0):.4f} | Precisão: {precision_score(y_test, y_pred_pp, zero_division=0):.4f} | Sensibilidade: {recall_score(y_test, y_pred_pp, zero_division=0):.4f}\")\n",
    "    print(\"Matriz de Confusão:\\n\", cm)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTempo total de execução: {(time.time()-start_time)/60:.2f} min\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bb4f5a",
   "metadata": {},
   "source": [
    "SVM, RANDOM FOREST, LOGISTIC REGRESSION, KNN, DECISION TREE, GRADIENT BOOSTING E NAIVE BAYES EM 5 PACIENTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf50e1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serviço do Google Drive conectado com sucesso.\n",
      "Caminho do dataset encontrado no Drive!\n",
      "\n",
      "=============== chb01 ===============\n",
      "Arquivo salvo em: ./temp_chb01_summary.txt\n",
      "chb01: Processando 7 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb01_03.edf\n",
      "Arquivo salvo em: ./temp_chb01_03.edf\n",
      "  Processando: chb01_04.edf\n",
      "Arquivo salvo em: ./temp_chb01_04.edf\n",
      "  Processando: chb01_15.edf\n",
      "Arquivo salvo em: ./temp_chb01_15.edf\n",
      "  Processando: chb01_16.edf\n",
      "Arquivo salvo em: ./temp_chb01_16.edf\n",
      "  Processando: chb01_18.edf\n",
      "Arquivo salvo em: ./temp_chb01_18.edf\n",
      "\n",
      ">>> Lote 2:\n",
      "  Processando: chb01_21.edf\n",
      "Arquivo salvo em: ./temp_chb01_21.edf\n",
      "  Processando: chb01_26.edf\n",
      "Arquivo salvo em: ./temp_chb01_26.edf\n",
      "\n",
      "=============== chb02 ===============\n",
      "Arquivo salvo em: ./temp_chb02_summary.txt\n",
      "chb02: Processando 3 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb02_16+.edf\n",
      "Arquivo salvo em: ./temp_chb02_16+.edf\n",
      "  Processando: chb02_16.edf\n",
      "Arquivo salvo em: ./temp_chb02_16.edf\n",
      "  Processando: chb02_19.edf\n",
      "Arquivo salvo em: ./temp_chb02_19.edf\n",
      "\n",
      "=============== chb03 ===============\n",
      "Arquivo salvo em: ./temp_chb03_summary.txt\n",
      "chb03: Processando 7 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb03_01.edf\n",
      "Arquivo salvo em: ./temp_chb03_01.edf\n",
      "  Processando: chb03_02.edf\n",
      "Arquivo salvo em: ./temp_chb03_02.edf\n",
      "  Processando: chb03_03.edf\n",
      "Arquivo salvo em: ./temp_chb03_03.edf\n",
      "  Processando: chb03_04.edf\n",
      "Arquivo salvo em: ./temp_chb03_04.edf\n",
      "  Processando: chb03_34.edf\n",
      "Arquivo salvo em: ./temp_chb03_34.edf\n",
      "\n",
      ">>> Lote 2:\n",
      "  Processando: chb03_35.edf\n",
      "Arquivo salvo em: ./temp_chb03_35.edf\n",
      "  Processando: chb03_36.edf\n",
      "Arquivo salvo em: ./temp_chb03_36.edf\n",
      "\n",
      "=============== chb04 ===============\n",
      "Arquivo salvo em: ./temp_chb04_summary.txt\n",
      "chb04: Processando 3 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb04_05.edf\n",
      "Arquivo salvo em: ./temp_chb04_05.edf\n",
      "  Processando: chb04_08.edf\n",
      "Arquivo salvo em: ./temp_chb04_08.edf\n",
      "  Processando: chb04_28.edf\n",
      "Arquivo salvo em: ./temp_chb04_28.edf\n",
      "\n",
      "=============== chb05 ===============\n",
      "Arquivo salvo em: ./temp_chb05_summary.txt\n",
      "chb05: Processando 5 arquivos com crise...\n",
      "\n",
      ">>> Lote 1:\n",
      "  Processando: chb05_06.edf\n",
      "Arquivo salvo em: ./temp_chb05_06.edf\n",
      "  Processando: chb05_13.edf\n",
      "Arquivo salvo em: ./temp_chb05_13.edf\n",
      "  Processando: chb05_16.edf\n",
      "Arquivo salvo em: ./temp_chb05_16.edf\n",
      "  Processando: chb05_17.edf\n",
      "Arquivo salvo em: ./temp_chb05_17.edf\n",
      "  Processando: chb05_22.edf\n",
      "Arquivo salvo em: ./temp_chb05_22.edf\n",
      "\n",
      "========================================\n",
      "    FASE DE TREINAMENTO E AVALIAÇÃO     \n",
      "========================================\n",
      "\n",
      "Frequência das janelas calculada: 1.00 Hz.\n",
      "\n",
      "--- Treinando e Avaliando: SVM ---\n",
      "F1: 0.6667 | Precisão: 0.5061 | Sensibilidade: 0.9764\n",
      "Matriz de Confusão:\n",
      " [[ 29 565]\n",
      " [ 14 579]]\n",
      "\n",
      "--- Treinando e Avaliando: RandomForest ---\n",
      "F1: 0.6667 | Precisão: 0.5061 | Sensibilidade: 0.9764\n",
      "Matriz de Confusão:\n",
      " [[ 29 565]\n",
      " [ 14 579]]\n",
      "\n",
      "--- Treinando e Avaliando: LogisticRegression ---\n",
      "F1: 0.6695 | Precisão: 0.5222 | Sensibilidade: 0.9325\n",
      "Matriz de Confusão:\n",
      " [[ 88 506]\n",
      " [ 40 553]]\n",
      "\n",
      "--- Treinando e Avaliando: KNN ---\n",
      "F1: 0.6671 | Precisão: 0.5112 | Sensibilidade: 0.9595\n",
      "Matriz de Confusão:\n",
      " [[ 50 544]\n",
      " [ 24 569]]\n",
      "\n",
      "--- Treinando e Avaliando: DecisionTree ---\n",
      "F1: 0.6675 | Precisão: 0.5197 | Sensibilidade: 0.9325\n",
      "Matriz de Confusão:\n",
      " [[ 83 511]\n",
      " [ 40 553]]\n",
      "\n",
      "--- Treinando e Avaliando: GradientBoosting ---\n",
      "F1: 0.6686 | Precisão: 0.5097 | Sensibilidade: 0.9713\n",
      "Matriz de Confusão:\n",
      " [[ 40 554]\n",
      " [ 17 576]]\n",
      "\n",
      "--- Treinando e Avaliando: NaiveBayes ---\n",
      "F1: 0.5532 | Precisão: 0.5450 | Sensibilidade: 0.5616\n",
      "Matriz de Confusão:\n",
      " [[316 278]\n",
      " [260 333]]\n",
      "\n",
      "Tempo total de execução: 207.84 min\n"
     ]
    }
   ],
   "source": [
    "PATIENTS = [f\"chb{str(i).zfill(2)}\" for i in range(1, 6)]\n",
    "DRIVE_PATH_COMPONENTS = ['TCC EPILEPSIA DATA', 'chb-mit-scalp-eeg-database-1.0.0']\n",
    "\n",
    "WINDOW_SECONDS = 2\n",
    "MAX_CHANNELS = 20 \n",
    "BATCH_SIZE = 5\n",
    "SEED = 42\n",
    "\n",
    "SMOOTHING_WINDOW_SIZE = 5\n",
    "MERGE_SEIZURES_THRESHOLD = 30\n",
    "\n",
    "start_time = time.time()\n",
    "try:\n",
    "    service = get_drive_service()\n",
    "    root_id = find_folder_id_by_path(service, DRIVE_PATH_COMPONENTS)\n",
    "\n",
    "    all_features, all_labels = [], []\n",
    "    fs_signal = -1\n",
    "\n",
    "    for patient in PATIENTS:\n",
    "        print(f\"\\n{'='*15} {patient} {'='*15}\")\n",
    "        patient_id = find_folder_id(service, patient, parent_id=root_id)\n",
    "        files_map = get_files_from_drive_folder(service, patient_id)\n",
    "\n",
    "        summary_path = f\"./temp_{patient}_summary.txt\"\n",
    "        download_file_locally(service, files_map[f\"{patient}-summary.txt\"], summary_path)\n",
    "        seiz = parse_summary_file(summary_path)\n",
    "        os.remove(summary_path)\n",
    "\n",
    "        edf_with_seiz = sorted([f for f, ivals in seiz.items() if len(ivals) > 0])\n",
    "        print(f\"{patient}: Processando {len(edf_with_seiz)} arquivos com crise...\")\n",
    "\n",
    "        for k in range(0, len(edf_with_seiz), BATCH_SIZE):\n",
    "            batch = edf_with_seiz[k:k+BATCH_SIZE]\n",
    "            print(f\"\\n>>> Lote {k//BATCH_SIZE + 1}:\")\n",
    "\n",
    "            for edf_name in batch:\n",
    "                local_path = f\"./temp_{edf_name}\"\n",
    "                print(f\"  Processando: {edf_name}\")\n",
    "                try:\n",
    "                    download_file_locally(service, files_map[edf_name], local_path)\n",
    "                    with pyedflib.EdfReader(local_path) as r:\n",
    "                        fs_signal = r.getSampleFrequency(0)\n",
    "                        n_ch = min(int(MAX_CHANNELS), r.signals_in_file)\n",
    "                        signals = np.array([r.readSignal(c) for c in range(n_ch)], dtype=np.float32)\n",
    "                        win_samples = int(fs_signal * WINDOW_SECONDS); step = win_samples // 2\n",
    "                        \n",
    "                        for j in range(0, signals.shape[1] - win_samples + 1, step):\n",
    "                            window = signals[:, j : j + win_samples]\n",
    "                            start_time_sec, end_time_sec = j / fs_signal, (j + win_samples) / fs_signal\n",
    "                            is_seiz = any(max(start_time_sec, s_start) < min(end_time_sec, s_end) for s_start, s_end in seiz.get(edf_name, []))\n",
    "                            feats = np.mean([extract_single_feature_vector(window[c, :], fs_signal) for c in range(n_ch)], axis=0)\n",
    "                            all_features.append(feats); all_labels.append(1 if is_seiz else 0)\n",
    "                except Exception as e: print(f\"  [ERRO] Falha ao processar {edf_name}: {e}\")\n",
    "                finally:\n",
    "                    if os.path.exists(local_path): os.remove(local_path)\n",
    "            gc.collect()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\" + \"FASE DE TREINAMENTO E AVALIAÇÃO\".center(40) + \"\\n\" + \"=\"*40)\n",
    "    \n",
    "    X = np.array(all_features); y = np.array(all_labels)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "    X_scaled = np.nan_to_num(X_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    X_pos, X_neg = X_scaled[y == 1], X_scaled[y == 0]\n",
    "    if len(X_pos) == 0: raise ValueError(\"Nenhuma amostra de crise foi encontrada nos dados processados.\")\n",
    "\n",
    "    X_neg_balanced = resample(X_neg, replace=False, n_samples=len(X_pos), random_state=SEED)\n",
    "    X_balanced = np.vstack([X_pos, X_neg_balanced])\n",
    "    \n",
    "    y_balanced = np.hstack([np.ones(len(X_pos)), np.zeros(len(X_pos))]).astype(int)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.3, stratify=y_balanced, random_state=SEED)\n",
    "\n",
    "    models = {\n",
    "        \"SVM\": SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", random_state=SEED),\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=SEED, n_jobs=-1),\n",
    "        \"LogisticRegression\": LogisticRegression(random_state=SEED, max_iter=1000, n_jobs=-1),\n",
    "        \"KNN\": KNeighborsClassifier(n_neighbors=7, n_jobs=-1),\n",
    "        \"DecisionTree\": DecisionTreeClassifier(random_state=SEED),\n",
    "        \"GradientBoosting\": GradientBoostingClassifier(n_estimators=100, random_state=SEED),\n",
    "        \"NaiveBayes\": GaussianNB()\n",
    "    }\n",
    "\n",
    "    win_samples_final = int(fs_signal * WINDOW_SECONDS); step_final = win_samples_final // 2\n",
    "    fs_windows = fs_signal / step_final\n",
    "    print(f\"\\nFrequência das janelas calculada: {fs_windows:.2f} Hz.\")\n",
    "\n",
    "    for name, clf in models.items():\n",
    "        print(f\"\\n--- Treinando e Avaliando: {name} ---\")\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_raw = clf.predict(X_test)\n",
    "        \n",
    "        y_pred_pp = post_process_predictions(y_pred_raw, window_size=SMOOTHING_WINDOW_SIZE,\n",
    "                                           merge_gap=MERGE_SEIZURES_THRESHOLD, fs=fs_windows)\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred_pp, labels=[0,1])\n",
    "        print(f\"F1: {f1_score(y_test, y_pred_pp, zero_division=0):.4f} | Precisão: {precision_score(y_test, y_pred_pp, zero_division=0):.4f} | Sensibilidade: {recall_score(y_test, y_pred_pp, zero_division=0):.4f}\")\n",
    "        print(\"Matriz de Confusão:\\n\", cm)\n",
    "\n",
    "except Exception as e:\n",
    "     raise e\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTempo total de execução: {(time.time()-start_time)/60:.2f} min\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
